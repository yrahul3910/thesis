running: {'--uuid': '063e6207ddab5db082f0e63fc0f5b6f2', '-db-root': '/home/ryedida/bbo_challenge_starter_kit/output', '--opt-root': '/home/ryedida/bbo_challenge_starter_kit/example_submissions', '--data-root': None, '--db': 'run_20230429_190633', '--opt': 'turbo', '--data': 'digits', '--classifier': 'MLP-adam', '--metric': 'acc', '--calls': 15, '--suggestions': 1, '--jobs-file': None, '--verbose': False, 'dry_run': False, 'rev': 'a376313', 'opt_rev': '0.0.1'}
cmd: python turbo/optimizer.py -c MLP-adam -d digits -o turbo -u 063e6207ddab5db082f0e63fc0f5b6f2 -m acc -n 15 -p 1 -dir /home/ryedida/bbo_challenge_starter_kit/output -b run_20230429_190633
/home/ryedida/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:605: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

/home/ryedida/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  warnings.warn(

computed signature: ([{'alpha': 0.019628224813442792, 'batch_size': 182, 'beta_1': 0.9410202200271762, 'beta_2': 0.9998021557676793, 'epsilon': 1.8662266976518e-08, 'hidden_layer_sizes': 147, 'learning_rate_init': 0.0005627932047415167, 'tol': 0.03690557729213761, 'validation_fraction': 0.8846827852548593}, {'alpha': 0.0019982467392329444, 'batch_size': 200, 'beta_1': 0.919111482530466, 'beta_2': 0.9998488260436156, 'epsilon': 5.981221901152555e-07, 'hidden_layer_sizes': 61, 'learning_rate_init': 2.2310905607443014e-05, 'tol': 1.2046852412030316e-05, 'validation_fraction': 0.8117896445826539}, {'alpha': 0.46659545670218433, 'batch_size': 219, 'beta_1': 0.9889789783750891, 'beta_2': 0.9999896868093284, 'epsilon': 2.4234724484675948e-08, 'hidden_layer_sizes': 167, 'learning_rate_init': 2.972334644335654e-05, 'tol': 0.0036281404040243792, 'validation_fraction': 0.17260651658522078}, {'alpha': 4.656005689076002, 'batch_size': 135, 'beta_1': 0.870503881627747, 'beta_2': 0.9948873266941017, 'epsilon': 2.1023308743480125e-07, 'hidden_layer_sizes': 118, 'learning_rate_init': 0.0018781738757161913, 'tol': 1.1889379831773004e-05, 'validation_fraction': 0.6264327093752792}, {'alpha': 0.04705159350400542, 'batch_size': 158, 'beta_1': 0.9870884262730957, 'beta_2': 0.9999596874382349, 'epsilon': 1.1981845126013875e-08, 'hidden_layer_sizes': 116, 'learning_rate_init': 0.00617340520407431, 'tol': 1.741413418158619e-05, 'validation_fraction': 0.6754299026638921}], [-0.21081107239643826, -0.1295489740611692, -0.27999177313201706, -0.9422522260936896, -0.9332050909794812])
/home/ryedida/.local/lib/python3.9/site-packages/bayesmark/signatures.py:85: RuntimeWarning: Signature diverged on MLP-adam_digits_acc betwen [-0.2181506  -0.12954897 -0.42701514 -0.94997127 -0.93320509] and [-0.21081107 -0.11575494 -0.27999177 -0.94225223 -0.93253861]
  warnings.warn(

Signature errors:
                           0         1         2         3         4       max
MLP-adam_digits_acc  0.00734  0.013794  0.147023  0.007719  0.000666  0.147023
max                  0.00734  0.013794  0.147023  0.007719  0.000666  0.147023
starting sklearn study turbo MLP-adam digits acc 15 1
with data root: None
suggestion time taken 0.002244 iter 0 next_points [{'alpha': 1.3854962663807484e-05, 'batch_size': 141, 'beta_1': 0.9643145822333583, 'beta_2': 0.999828298597762, 'epsilon': 6.185735119728057e-09, 'hidden_layer_sizes': 192, 'learning_rate_init': 7.275726944764709e-05, 'tol': 0.0007706659550189317, 'validation_fraction': 0.3483829631791682}]
function_evaluation time 4.173776 value -0.761087 suggestion {'alpha': 1.3854962663807484e-05, 'batch_size': 141, 'beta_1': 0.9643145822333583, 'beta_2': 0.999828298597762, 'epsilon': 6.185735119728057e-09, 'hidden_layer_sizes': 192, 'learning_rate_init': 7.275726944764709e-05, 'tol': 0.0007706659550189317, 'validation_fraction': 0.3483829631791682}
observation time 0.001519, current best -0.761087 at iter 0
suggestion time taken 0.001813 iter 1 next_points [{'alpha': 0.02157270414541359, 'batch_size': 100, 'beta_1': 0.6549972010336004, 'beta_2': 0.9946822942220344, 'epsilon': 1.0046741366236678e-09, 'hidden_layer_sizes': 77, 'learning_rate_init': 0.0004525816945486815, 'tol': 0.001154107553377191, 'validation_fraction': 0.13538068614685067}]
function_evaluation time 1.386341 value -0.933902 suggestion {'alpha': 0.02157270414541359, 'batch_size': 100, 'beta_1': 0.6549972010336004, 'beta_2': 0.9946822942220344, 'epsilon': 1.0046741366236678e-09, 'hidden_layer_sizes': 77, 'learning_rate_init': 0.0004525816945486815, 'tol': 0.001154107553377191, 'validation_fraction': 0.13538068614685067}
observation time 0.001430, current best -0.933902 at iter 1
suggestion time taken 0.001847 iter 2 next_points [{'alpha': 4.349475664848766e-05, 'batch_size': 92, 'beta_1': 0.9766306723794235, 'beta_2': 0.9999611417274562, 'epsilon': 3.0167574405735935e-09, 'hidden_layer_sizes': 141, 'learning_rate_init': 3.855235440600347e-05, 'tol': 9.251420358658359e-05, 'validation_fraction': 0.26608308272778286}]
/home/ryedida/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  warnings.warn(

function_evaluation time 6.615733 value -0.889334 suggestion {'alpha': 4.349475664848766e-05, 'batch_size': 92, 'beta_1': 0.9766306723794235, 'beta_2': 0.9999611417274562, 'epsilon': 3.0167574405735935e-09, 'hidden_layer_sizes': 141, 'learning_rate_init': 3.855235440600347e-05, 'tol': 9.251420358658359e-05, 'validation_fraction': 0.26608308272778286}
observation time 0.001482, current best -0.933902 at iter 2
suggestion time taken 0.001809 iter 3 next_points [{'alpha': 0.542351480202901, 'batch_size': 32, 'beta_1': 0.7633924334442813, 'beta_2': 0.9999987257150498, 'epsilon': 1.3739457857360104e-08, 'hidden_layer_sizes': 176, 'learning_rate_init': 0.03989471872908342, 'tol': 0.00030332499702204137, 'validation_fraction': 0.435386001407832}]
function_evaluation time 1.279476 value -0.924833 suggestion {'alpha': 0.542351480202901, 'batch_size': 32, 'beta_1': 0.7633924334442813, 'beta_2': 0.9999987257150498, 'epsilon': 1.3739457857360104e-08, 'hidden_layer_sizes': 176, 'learning_rate_init': 0.03989471872908342, 'tol': 0.00030332499702204137, 'validation_fraction': 0.435386001407832}
observation time 0.001429, current best -0.933902 at iter 3
suggestion time taken 0.001836 iter 4 next_points [{'alpha': 0.007930497748141974, 'batch_size': 113, 'beta_1': 0.8780487331191578, 'beta_2': 0.9999291716797877, 'epsilon': 1.295570014248614e-07, 'hidden_layer_sizes': 107, 'learning_rate_init': 0.00523928739628957, 'tol': 6.895205229988128e-05, 'validation_fraction': 0.6114511198338175}]
function_evaluation time 0.664049 value -0.946431 suggestion {'alpha': 0.007930497748141974, 'batch_size': 113, 'beta_1': 0.8780487331191578, 'beta_2': 0.9999291716797877, 'epsilon': 1.295570014248614e-07, 'hidden_layer_sizes': 107, 'learning_rate_init': 0.00523928739628957, 'tol': 6.895205229988128e-05, 'validation_fraction': 0.6114511198338175}
observation time 0.001503, current best -0.946431 at iter 4
suggestion time taken 0.001830 iter 5 next_points [{'alpha': 0.001628575570647059, 'batch_size': 133, 'beta_1': 0.5088544365762936, 'beta_2': 0.9966680428566879, 'epsilon': 6.180624728046095e-07, 'hidden_layer_sizes': 188, 'learning_rate_init': 0.07767144487256039, 'tol': 0.012590669884277388, 'validation_fraction': 0.161471336731586}]
function_evaluation time 0.912018 value -0.657453 suggestion {'alpha': 0.001628575570647059, 'batch_size': 133, 'beta_1': 0.5088544365762936, 'beta_2': 0.9966680428566879, 'epsilon': 6.180624728046095e-07, 'hidden_layer_sizes': 188, 'learning_rate_init': 0.07767144487256039, 'tol': 0.012590669884277388, 'validation_fraction': 0.161471336731586}
observation time 0.001420, current best -0.946431 at iter 5
suggestion time taken 0.001822 iter 6 next_points [{'alpha': 0.25045998661570573, 'batch_size': 56, 'beta_1': 0.9298352288423183, 'beta_2': 0.9996255063480892, 'epsilon': 1.791467029996906e-09, 'hidden_layer_sizes': 98, 'learning_rate_init': 0.0023653185426074875, 'tol': 0.005596821945318808, 'validation_fraction': 0.4015824670834359}]
function_evaluation time 0.781618 value -0.954759 suggestion {'alpha': 0.25045998661570573, 'batch_size': 56, 'beta_1': 0.9298352288423183, 'beta_2': 0.9996255063480892, 'epsilon': 1.791467029996906e-09, 'hidden_layer_sizes': 98, 'learning_rate_init': 0.0023653185426074875, 'tol': 0.005596821945318808, 'validation_fraction': 0.4015824670834359}
observation time 0.001458, current best -0.954759 at iter 6
suggestion time taken 0.001821 iter 7 next_points [{'alpha': 0.0002695739103636835, 'batch_size': 181, 'beta_1': 0.9154559154544262, 'beta_2': 0.9797462966861479, 'epsilon': 9.53666131468408e-08, 'hidden_layer_sizes': 134, 'learning_rate_init': 1.3350021577902065e-05, 'tol': 0.003202028167073474, 'validation_fraction': 0.10903714640232129}]
/home/ryedida/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  warnings.warn(

function_evaluation time 1.855678 value -0.160724 suggestion {'alpha': 0.0002695739103636835, 'batch_size': 181, 'beta_1': 0.9154559154544262, 'beta_2': 0.9797462966861479, 'epsilon': 9.53666131468408e-08, 'hidden_layer_sizes': 134, 'learning_rate_init': 1.3350021577902065e-05, 'tol': 0.003202028167073474, 'validation_fraction': 0.10903714640232129}
observation time 0.001422, current best -0.954759 at iter 7
suggestion time taken 0.001865 iter 8 next_points [{'alpha': 0.0013189556207455432, 'batch_size': 19, 'beta_1': 0.870811372863009, 'beta_2': 0.9992873233912715, 'epsilon': 2.2521147466446305e-08, 'hidden_layer_sizes': 56, 'learning_rate_init': 0.036125188358256274, 'tol': 0.01647251360733242, 'validation_fraction': 0.2495926293511435}]
function_evaluation time 1.288546 value -0.929000 suggestion {'alpha': 0.0013189556207455432, 'batch_size': 19, 'beta_1': 0.870811372863009, 'beta_2': 0.9992873233912715, 'epsilon': 2.2521147466446305e-08, 'hidden_layer_sizes': 56, 'learning_rate_init': 0.036125188358256274, 'tol': 0.01647251360733242, 'validation_fraction': 0.2495926293511435}
observation time 0.001436, current best -0.954759 at iter 8
suggestion time taken 0.002209 iter 9 next_points [{'alpha': 0.0004303704989550495, 'batch_size': 218, 'beta_1': 0.9897289923228888, 'beta_2': 0.9320081769512308, 'epsilon': 4.831413504140662e-08, 'hidden_layer_sizes': 60, 'learning_rate_init': 0.020789019954969384, 'tol': 1.2353805086077566e-05, 'validation_fraction': 0.47636107480900025}]
function_evaluation time 0.744384 value -0.926236 suggestion {'alpha': 0.0004303704989550495, 'batch_size': 218, 'beta_1': 0.9897289923228888, 'beta_2': 0.9320081769512308, 'epsilon': 4.831413504140662e-08, 'hidden_layer_sizes': 60, 'learning_rate_init': 0.020789019954969384, 'tol': 1.2353805086077566e-05, 'validation_fraction': 0.47636107480900025}
observation time 0.001415, current best -0.954759 at iter 9
suggestion time taken 0.001796 iter 10 next_points [{'alpha': 4.111896039325458, 'batch_size': 83, 'beta_1': 0.566853641230691, 'beta_2': 0.9999966086060926, 'epsilon': 2.5876420577670876e-09, 'hidden_layer_sizes': 145, 'learning_rate_init': 0.00011402056457811069, 'tol': 0.0015875052946918678, 'validation_fraction': 0.7577488195139903}]
function_evaluation time 3.016308 value -0.863608 suggestion {'alpha': 4.111896039325458, 'batch_size': 83, 'beta_1': 0.566853641230691, 'beta_2': 0.9999966086060926, 'epsilon': 2.5876420577670876e-09, 'hidden_layer_sizes': 145, 'learning_rate_init': 0.00011402056457811069, 'tol': 0.0015875052946918678, 'validation_fraction': 0.7577488195139903}
observation time 0.001460, current best -0.954759 at iter 10
suggestion time taken 0.001847 iter 11 next_points [{'alpha': 8.160023880028012, 'batch_size': 36, 'beta_1': 0.9562704948902195, 'beta_2': 0.9643702487010175, 'epsilon': 2.8078219526755356e-08, 'hidden_layer_sizes': 88, 'learning_rate_init': 0.0017337897896531433, 'tol': 0.0050626869214076735, 'validation_fraction': 0.8983508970149499}]
function_evaluation time 0.611774 value -0.877519 suggestion {'alpha': 8.160023880028012, 'batch_size': 36, 'beta_1': 0.9562704948902195, 'beta_2': 0.9643702487010175, 'epsilon': 2.8078219526755356e-08, 'hidden_layer_sizes': 88, 'learning_rate_init': 0.0017337897896531433, 'tol': 0.0050626869214076735, 'validation_fraction': 0.8983508970149499}
observation time 0.001428, current best -0.954759 at iter 11
suggestion time taken 0.001808 iter 12 next_points [{'alpha': 8.919391791128588e-05, 'batch_size': 167, 'beta_1': 0.969478859046031, 'beta_2': 0.9981834296589553, 'epsilon': 1.0313797023449709e-08, 'hidden_layer_sizes': 91, 'learning_rate_init': 0.001126567144253637, 'tol': 0.04994814412638876, 'validation_fraction': 0.6514452023396052}]
function_evaluation time 0.406615 value -0.858708 suggestion {'alpha': 8.919391791128588e-05, 'batch_size': 167, 'beta_1': 0.969478859046031, 'beta_2': 0.9981834296589553, 'epsilon': 1.0313797023449709e-08, 'hidden_layer_sizes': 91, 'learning_rate_init': 0.001126567144253637, 'tol': 0.04994814412638876, 'validation_fraction': 0.6514452023396052}
observation time 0.001681, current best -0.954759 at iter 12
suggestion time taken 0.001871 iter 13 next_points [{'alpha': 0.7046563496955178, 'batch_size': 231, 'beta_1': 0.8390951207942491, 'beta_2': 0.9999915313595359, 'epsilon': 3.9696977405862163e-07, 'hidden_layer_sizes': 123, 'learning_rate_init': 0.013318460205524673, 'tol': 0.00016678887948343366, 'validation_fraction': 0.8045091707714107}]
function_evaluation time 0.483315 value -0.917182 suggestion {'alpha': 0.7046563496955178, 'batch_size': 231, 'beta_1': 0.8390951207942491, 'beta_2': 0.9999915313595359, 'epsilon': 3.9696977405862163e-07, 'hidden_layer_sizes': 123, 'learning_rate_init': 0.013318460205524673, 'tol': 0.00016678887948343366, 'validation_fraction': 0.8045091707714107}
observation time 0.001509, current best -0.954759 at iter 13
suggestion time taken 0.001997 iter 14 next_points [{'alpha': 2.3050748683479153e-05, 'batch_size': 211, 'beta_1': 0.9428261977541964, 'beta_2': 0.9835499984714489, 'epsilon': 4.379818905897426e-09, 'hidden_layer_sizes': 114, 'learning_rate_init': 1.79286989670158e-05, 'tol': 0.000200072769136747, 'validation_fraction': 0.8718902826419143}]
/home/ryedida/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:605: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

function_evaluation time 0.285463 value -0.108599 suggestion {'alpha': 2.3050748683479153e-05, 'batch_size': 211, 'beta_1': 0.9428261977541964, 'beta_2': 0.9835499984714489, 'epsilon': 4.379818905897426e-09, 'hidden_layer_sizes': 114, 'learning_rate_init': 1.79286989670158e-05, 'tol': 0.000200072769136747, 'validation_fraction': 0.8718902826419143}
observation time 0.001398, current best -0.954759 at iter 14
saving meta data: {'args': {'--uuid': '063e6207ddab5db082f0e63fc0f5b6f2', '-db-root': '/home/ryedida/bbo_challenge_starter_kit/output', '--opt-root': '/home/ryedida/bbo_challenge_starter_kit/example_submissions', '--data-root': None, '--db': 'run_20230429_190633', '--opt': 'turbo', '--data': 'digits', '--classifier': 'MLP-adam', '--metric': 'acc', '--calls': 15, '--suggestions': 1, '--jobs-file': None, '--verbose': False, 'dry_run': False, 'rev': 'a376313', 'opt_rev': '0.0.1'}, 'signature': ([{'alpha': 0.019628224813442792, 'batch_size': 182, 'beta_1': 0.9410202200271762, 'beta_2': 0.9998021557676793, 'epsilon': 1.8662266976518e-08, 'hidden_layer_sizes': 147, 'learning_rate_init': 0.0005627932047415167, 'tol': 0.03690557729213761, 'validation_fraction': 0.8846827852548593}, {'alpha': 0.0019982467392329444, 'batch_size': 200, 'beta_1': 0.919111482530466, 'beta_2': 0.9998488260436156, 'epsilon': 5.981221901152555e-07, 'hidden_layer_sizes': 61, 'learning_rate_init': 2.2310905607443014e-05, 'tol': 1.2046852412030316e-05, 'validation_fraction': 0.8117896445826539}, {'alpha': 0.46659545670218433, 'batch_size': 219, 'beta_1': 0.9889789783750891, 'beta_2': 0.9999896868093284, 'epsilon': 2.4234724484675948e-08, 'hidden_layer_sizes': 167, 'learning_rate_init': 2.972334644335654e-05, 'tol': 0.0036281404040243792, 'validation_fraction': 0.17260651658522078}, {'alpha': 4.656005689076002, 'batch_size': 135, 'beta_1': 0.870503881627747, 'beta_2': 0.9948873266941017, 'epsilon': 2.1023308743480125e-07, 'hidden_layer_sizes': 118, 'learning_rate_init': 0.0018781738757161913, 'tol': 1.1889379831773004e-05, 'validation_fraction': 0.6264327093752792}, {'alpha': 0.04705159350400542, 'batch_size': 158, 'beta_1': 0.9870884262730957, 'beta_2': 0.9999596874382349, 'epsilon': 1.1981845126013875e-08, 'hidden_layer_sizes': 116, 'learning_rate_init': 0.00617340520407431, 'tol': 1.741413418158619e-05, 'validation_fraction': 0.6754299026638921}], [-0.21081107239643826, -0.1295489740611692, -0.27999177313201706, -0.9422522260936896, -0.9332050909794812])}
saving results
saving timing
saving suggest log
done
