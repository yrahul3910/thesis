running: {'--uuid': '1eb2ed5d1e155e4eb58cfee3c75e00a8', '-db-root': '/home/ryedida/bbo_challenge_starter_kit/output', '--opt-root': '/home/ryedida/bbo_challenge_starter_kit/example_submissions', '--data-root': None, '--db': 'run_20230430_001614', '--opt': 'hyperopt', '--data': 'digits', '--classifier': 'MLP-adam', '--metric': 'acc', '--calls': 15, '--suggestions': 1, '--jobs-file': None, '--verbose': False, 'dry_run': False, 'rev': 'a376313', 'opt_rev': '0.2.7'}
cmd: python hyperopt/optimizer.py -c MLP-adam -d digits -o hyperopt -u 1eb2ed5d1e155e4eb58cfee3c75e00a8 -m acc -n 15 -p 1 -dir /home/ryedida/bbo_challenge_starter_kit/output -b run_20230430_001614
/home/ryedida/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:605: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

/home/ryedida/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  warnings.warn(

computed signature: ([{'alpha': 0.019628224813442792, 'batch_size': 182, 'beta_1': 0.9410202200271762, 'beta_2': 0.9998021557676793, 'epsilon': 1.8662266976518e-08, 'hidden_layer_sizes': 147, 'learning_rate_init': 0.0005627932047415167, 'tol': 0.03690557729213761, 'validation_fraction': 0.8846827852548593}, {'alpha': 0.0019982467392329444, 'batch_size': 200, 'beta_1': 0.919111482530466, 'beta_2': 0.9998488260436156, 'epsilon': 5.981221901152555e-07, 'hidden_layer_sizes': 61, 'learning_rate_init': 2.2310905607443014e-05, 'tol': 1.2046852412030316e-05, 'validation_fraction': 0.8117896445826539}, {'alpha': 0.46659545670218433, 'batch_size': 219, 'beta_1': 0.9889789783750891, 'beta_2': 0.9999896868093284, 'epsilon': 2.4234724484675948e-08, 'hidden_layer_sizes': 167, 'learning_rate_init': 2.972334644335654e-05, 'tol': 0.0036281404040243792, 'validation_fraction': 0.17260651658522078}, {'alpha': 4.656005689076002, 'batch_size': 135, 'beta_1': 0.870503881627747, 'beta_2': 0.9948873266941017, 'epsilon': 2.1023308743480125e-07, 'hidden_layer_sizes': 118, 'learning_rate_init': 0.0018781738757161913, 'tol': 1.1889379831773004e-05, 'validation_fraction': 0.6264327093752792}, {'alpha': 0.04705159350400542, 'batch_size': 158, 'beta_1': 0.9870884262730957, 'beta_2': 0.9999596874382349, 'epsilon': 1.1981845126013875e-08, 'hidden_layer_sizes': 116, 'learning_rate_init': 0.00617340520407431, 'tol': 1.741413418158619e-05, 'validation_fraction': 0.6754299026638921}], [-0.21081107239643826, -0.1295489740611692, -0.27999177313201706, -0.9422522260936896, -0.9332050909794812])
/home/ryedida/.local/lib/python3.9/site-packages/bayesmark/signatures.py:85: RuntimeWarning: Signature diverged on MLP-adam_digits_acc betwen [-0.2181506  -0.12954897 -0.42701514 -0.94997127 -0.93320509] and [-0.21081107 -0.11575494 -0.27999177 -0.94225223 -0.93253861]
  warnings.warn(

Signature errors:
                           0         1         2         3         4       max
MLP-adam_digits_acc  0.00734  0.013794  0.147023  0.007719  0.000666  0.147023
max                  0.00734  0.013794  0.147023  0.007719  0.000666  0.147023
starting sklearn study hyperopt MLP-adam digits acc 15 1
with data root: None
suggestion time taken 0.002378 iter 0 next_points [{'alpha': 0.11142373437629524, 'batch_size': 120, 'beta_1': 0.9169320766743012, 'beta_2': 0.9284406356253683, 'epsilon': 1.6235184031197698e-07, 'hidden_layer_sizes': 154, 'learning_rate_init': 0.01041396164819011, 'tol': 0.016027224371950536, 'validation_fraction': 0.19078651700058374}]
function_evaluation time 0.617933 value -0.965914 suggestion {'alpha': 0.11142373437629524, 'batch_size': 120, 'beta_1': 0.9169320766743012, 'beta_2': 0.9284406356253683, 'epsilon': 1.6235184031197698e-07, 'hidden_layer_sizes': 154, 'learning_rate_init': 0.01041396164819011, 'tol': 0.016027224371950536, 'validation_fraction': 0.19078651700058374}
observation time 0.000080, current best -0.965914 at iter 0
suggestion time taken 0.002366 iter 1 next_points [{'alpha': 0.0015746091342128832, 'batch_size': 88, 'beta_1': 0.7891617640939258, 'beta_2': 0.9059733642978428, 'epsilon': 6.395213736259987e-08, 'hidden_layer_sizes': 195, 'learning_rate_init': 0.029658671189478642, 'tol': 0.0017823738847509778, 'validation_fraction': 0.14861750132873242}]
function_evaluation time 1.167936 value -0.946412 suggestion {'alpha': 0.0015746091342128832, 'batch_size': 88, 'beta_1': 0.7891617640939258, 'beta_2': 0.9059733642978428, 'epsilon': 6.395213736259987e-08, 'hidden_layer_sizes': 195, 'learning_rate_init': 0.029658671189478642, 'tol': 0.0017823738847509778, 'validation_fraction': 0.14861750132873242}
observation time 0.000071, current best -0.965914 at iter 1
suggestion time taken 0.002072 iter 2 next_points [{'alpha': 1.0213753442800905e-05, 'batch_size': 161, 'beta_1': 0.9072919280634081, 'beta_2': 0.9157757713689417, 'epsilon': 4.2377256601321605e-08, 'hidden_layer_sizes': 157, 'learning_rate_init': 0.011172894952490381, 'tol': 0.030791685533425515, 'validation_fraction': 0.14740711076504576}]
function_evaluation time 0.543821 value -0.958258 suggestion {'alpha': 1.0213753442800905e-05, 'batch_size': 161, 'beta_1': 0.9072919280634081, 'beta_2': 0.9157757713689417, 'epsilon': 4.2377256601321605e-08, 'hidden_layer_sizes': 157, 'learning_rate_init': 0.011172894952490381, 'tol': 0.030791685533425515, 'validation_fraction': 0.14740711076504576}
observation time 0.000073, current best -0.965914 at iter 2
suggestion time taken 0.002122 iter 3 next_points [{'alpha': 0.0003019121858405307, 'batch_size': 196, 'beta_1': 0.5007368128428081, 'beta_2': 0.9964765870122355, 'epsilon': 6.406650350326936e-09, 'hidden_layer_sizes': 162, 'learning_rate_init': 3.1426874421068424e-05, 'tol': 0.014602030934280946, 'validation_fraction': 0.1781298239879159}]
function_evaluation time 0.414730 value -0.135034 suggestion {'alpha': 0.0003019121858405307, 'batch_size': 196, 'beta_1': 0.5007368128428081, 'beta_2': 0.9964765870122355, 'epsilon': 6.406650350326936e-09, 'hidden_layer_sizes': 162, 'learning_rate_init': 3.1426874421068424e-05, 'tol': 0.014602030934280946, 'validation_fraction': 0.1781298239879159}
observation time 0.000069, current best -0.965914 at iter 3
suggestion time taken 0.002146 iter 4 next_points [{'alpha': 4.738890567833149, 'batch_size': 198, 'beta_1': 0.6814949974565931, 'beta_2': 0.9297725764317735, 'epsilon': 1.3962060783833218e-09, 'hidden_layer_sizes': 51, 'learning_rate_init': 0.00030076962039380224, 'tol': 5.409812874322639e-05, 'validation_fraction': 0.1837422794706011}]
function_evaluation time 1.249798 value -0.936685 suggestion {'alpha': 4.738890567833149, 'batch_size': 198, 'beta_1': 0.6814949974565931, 'beta_2': 0.9297725764317735, 'epsilon': 1.3962060783833218e-09, 'hidden_layer_sizes': 51, 'learning_rate_init': 0.00030076962039380224, 'tol': 5.409812874322639e-05, 'validation_fraction': 0.1837422794706011}
observation time 0.000072, current best -0.965914 at iter 4
suggestion time taken 0.002165 iter 5 next_points [{'alpha': 2.9299073819612466e-05, 'batch_size': 174, 'beta_1': 0.7691342831483179, 'beta_2': 0.9179675297056055, 'epsilon': 1.1462287355862574e-07, 'hidden_layer_sizes': 79, 'learning_rate_init': 6.57080219232248e-05, 'tol': 0.0008501991002399812, 'validation_fraction': 0.1453379836369589}]
function_evaluation time 2.592525 value -0.708057 suggestion {'alpha': 2.9299073819612466e-05, 'batch_size': 174, 'beta_1': 0.7691342831483179, 'beta_2': 0.9179675297056055, 'epsilon': 1.1462287355862574e-07, 'hidden_layer_sizes': 79, 'learning_rate_init': 6.57080219232248e-05, 'tol': 0.0008501991002399812, 'validation_fraction': 0.1453379836369589}
observation time 0.000079, current best -0.965914 at iter 5
suggestion time taken 0.002134 iter 6 next_points [{'alpha': 0.0012936148844579153, 'batch_size': 30, 'beta_1': 0.5449366028771967, 'beta_2': 0.9943444560261216, 'epsilon': 2.513406103134169e-07, 'hidden_layer_sizes': 142, 'learning_rate_init': 0.0021545450145290944, 'tol': 0.0015963130779605567, 'validation_fraction': 0.12133154892945097}]
function_evaluation time 1.939182 value -0.968690 suggestion {'alpha': 0.0012936148844579153, 'batch_size': 30, 'beta_1': 0.5449366028771967, 'beta_2': 0.9943444560261216, 'epsilon': 2.513406103134169e-07, 'hidden_layer_sizes': 142, 'learning_rate_init': 0.0021545450145290944, 'tol': 0.0015963130779605567, 'validation_fraction': 0.12133154892945097}
observation time 0.000071, current best -0.968690 at iter 6
suggestion time taken 0.002437 iter 7 next_points [{'alpha': 0.0006159058136107267, 'batch_size': 212, 'beta_1': 0.8538575560791989, 'beta_2': 0.9534822783082337, 'epsilon': 9.805831012893672e-09, 'hidden_layer_sizes': 110, 'learning_rate_init': 0.0019172633394711589, 'tol': 0.03171428883048702, 'validation_fraction': 0.3539622044564493}]
function_evaluation time 0.408191 value -0.942235 suggestion {'alpha': 0.0006159058136107267, 'batch_size': 212, 'beta_1': 0.8538575560791989, 'beta_2': 0.9534822783082337, 'epsilon': 9.805831012893672e-09, 'hidden_layer_sizes': 110, 'learning_rate_init': 0.0019172633394711589, 'tol': 0.03171428883048702, 'validation_fraction': 0.3539622044564493}
observation time 0.000073, current best -0.968690 at iter 7
suggestion time taken 0.002264 iter 8 next_points [{'alpha': 3.789757199951224, 'batch_size': 219, 'beta_1': 0.7250543334039372, 'beta_2': 0.9400619345098604, 'epsilon': 4.0974734893613487e-07, 'hidden_layer_sizes': 197, 'learning_rate_init': 0.004983315698406486, 'tol': 3.491901693762163e-05, 'validation_fraction': 0.6391906573819531}]
function_evaluation time 0.849757 value -0.949898 suggestion {'alpha': 3.789757199951224, 'batch_size': 219, 'beta_1': 0.7250543334039372, 'beta_2': 0.9400619345098604, 'epsilon': 4.0974734893613487e-07, 'hidden_layer_sizes': 197, 'learning_rate_init': 0.004983315698406486, 'tol': 3.491901693762163e-05, 'validation_fraction': 0.6391906573819531}
observation time 0.000071, current best -0.968690 at iter 8
suggestion time taken 0.002343 iter 9 next_points [{'alpha': 0.0015360849638241943, 'batch_size': 166, 'beta_1': 0.8876687865265349, 'beta_2': 0.979656587988415, 'epsilon': 4.662177561840541e-09, 'hidden_layer_sizes': 69, 'learning_rate_init': 0.0034374580126757827, 'tol': 0.0003292767230515404, 'validation_fraction': 0.417806814251314}]
function_evaluation time 1.153566 value -0.958946 suggestion {'alpha': 0.0015360849638241943, 'batch_size': 166, 'beta_1': 0.8876687865265349, 'beta_2': 0.979656587988415, 'epsilon': 4.662177561840541e-09, 'hidden_layer_sizes': 69, 'learning_rate_init': 0.0034374580126757827, 'tol': 0.0003292767230515404, 'validation_fraction': 0.417806814251314}
observation time 0.000075, current best -0.968690 at iter 9
suggestion time taken 0.002094 iter 10 next_points [{'alpha': 0.015728669715227028, 'batch_size': 196, 'beta_1': 0.5897767266978257, 'beta_2': 0.9537088372639771, 'epsilon': 2.9452568022193775e-07, 'hidden_layer_sizes': 140, 'learning_rate_init': 0.008230061186462577, 'tol': 0.05801894018814559, 'validation_fraction': 0.8662233991096238}]
/home/ryedida/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:605: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

function_evaluation time 0.249553 value -0.875426 suggestion {'alpha': 0.015728669715227028, 'batch_size': 196, 'beta_1': 0.5897767266978257, 'beta_2': 0.9537088372639771, 'epsilon': 2.9452568022193775e-07, 'hidden_layer_sizes': 140, 'learning_rate_init': 0.008230061186462577, 'tol': 0.05801894018814559, 'validation_fraction': 0.8662233991096238}
observation time 0.000076, current best -0.968690 at iter 10
suggestion time taken 0.002129 iter 11 next_points [{'alpha': 2.0847838980640856e-05, 'batch_size': 117, 'beta_1': 0.8598406182812863, 'beta_2': 0.912805755368347, 'epsilon': 3.741035087185337e-08, 'hidden_layer_sizes': 166, 'learning_rate_init': 0.02776425197323787, 'tol': 1.1800223833877335e-05, 'validation_fraction': 0.15314597811737585}]
function_evaluation time 1.021044 value -0.953375 suggestion {'alpha': 2.0847838980640856e-05, 'batch_size': 117, 'beta_1': 0.8598406182812863, 'beta_2': 0.912805755368347, 'epsilon': 3.741035087185337e-08, 'hidden_layer_sizes': 166, 'learning_rate_init': 0.02776425197323787, 'tol': 1.1800223833877335e-05, 'validation_fraction': 0.15314597811737585}
observation time 0.000077, current best -0.968690 at iter 11
suggestion time taken 0.002165 iter 12 next_points [{'alpha': 0.39099215607656457, 'batch_size': 148, 'beta_1': 0.8500138087913564, 'beta_2': 0.9467717164642641, 'epsilon': 3.9077191258558495e-07, 'hidden_layer_sizes': 137, 'learning_rate_init': 0.004822546716139314, 'tol': 1.1429957594565261e-05, 'validation_fraction': 0.14605324901112493}]
function_evaluation time 0.779601 value -0.963811 suggestion {'alpha': 0.39099215607656457, 'batch_size': 148, 'beta_1': 0.8500138087913564, 'beta_2': 0.9467717164642641, 'epsilon': 3.9077191258558495e-07, 'hidden_layer_sizes': 137, 'learning_rate_init': 0.004822546716139314, 'tol': 1.1429957594565261e-05, 'validation_fraction': 0.14605324901112493}
observation time 0.000074, current best -0.968690 at iter 12
suggestion time taken 0.002184 iter 13 next_points [{'alpha': 0.16578194782699168, 'batch_size': 21, 'beta_1': 0.5685793530689021, 'beta_2': 0.928670295035501, 'epsilon': 2.3306416577799453e-08, 'hidden_layer_sizes': 54, 'learning_rate_init': 0.0004060690840393353, 'tol': 0.00035099679863224276, 'validation_fraction': 0.27692752769315626}]
function_evaluation time 3.036996 value -0.958950 suggestion {'alpha': 0.16578194782699168, 'batch_size': 21, 'beta_1': 0.5685793530689021, 'beta_2': 0.928670295035501, 'epsilon': 2.3306416577799453e-08, 'hidden_layer_sizes': 54, 'learning_rate_init': 0.0004060690840393353, 'tol': 0.00035099679863224276, 'validation_fraction': 0.27692752769315626}
observation time 0.000076, current best -0.968690 at iter 13
suggestion time taken 0.002137 iter 14 next_points [{'alpha': 0.06823944528850175, 'batch_size': 32, 'beta_1': 0.5945797955769306, 'beta_2': 0.9734785533291122, 'epsilon': 2.6559391622225894e-09, 'hidden_layer_sizes': 145, 'learning_rate_init': 0.002622402866624001, 'tol': 0.05437537784361512, 'validation_fraction': 0.18752426071298187}]
function_evaluation time 1.086668 value -0.961043 suggestion {'alpha': 0.06823944528850175, 'batch_size': 32, 'beta_1': 0.5945797955769306, 'beta_2': 0.9734785533291122, 'epsilon': 2.6559391622225894e-09, 'hidden_layer_sizes': 145, 'learning_rate_init': 0.002622402866624001, 'tol': 0.05437537784361512, 'validation_fraction': 0.18752426071298187}
observation time 0.000081, current best -0.968690 at iter 14
saving meta data: {'args': {'--uuid': '1eb2ed5d1e155e4eb58cfee3c75e00a8', '-db-root': '/home/ryedida/bbo_challenge_starter_kit/output', '--opt-root': '/home/ryedida/bbo_challenge_starter_kit/example_submissions', '--data-root': None, '--db': 'run_20230430_001614', '--opt': 'hyperopt', '--data': 'digits', '--classifier': 'MLP-adam', '--metric': 'acc', '--calls': 15, '--suggestions': 1, '--jobs-file': None, '--verbose': False, 'dry_run': False, 'rev': 'a376313', 'opt_rev': '0.2.7'}, 'signature': ([{'alpha': 0.019628224813442792, 'batch_size': 182, 'beta_1': 0.9410202200271762, 'beta_2': 0.9998021557676793, 'epsilon': 1.8662266976518e-08, 'hidden_layer_sizes': 147, 'learning_rate_init': 0.0005627932047415167, 'tol': 0.03690557729213761, 'validation_fraction': 0.8846827852548593}, {'alpha': 0.0019982467392329444, 'batch_size': 200, 'beta_1': 0.919111482530466, 'beta_2': 0.9998488260436156, 'epsilon': 5.981221901152555e-07, 'hidden_layer_sizes': 61, 'learning_rate_init': 2.2310905607443014e-05, 'tol': 1.2046852412030316e-05, 'validation_fraction': 0.8117896445826539}, {'alpha': 0.46659545670218433, 'batch_size': 219, 'beta_1': 0.9889789783750891, 'beta_2': 0.9999896868093284, 'epsilon': 2.4234724484675948e-08, 'hidden_layer_sizes': 167, 'learning_rate_init': 2.972334644335654e-05, 'tol': 0.0036281404040243792, 'validation_fraction': 0.17260651658522078}, {'alpha': 4.656005689076002, 'batch_size': 135, 'beta_1': 0.870503881627747, 'beta_2': 0.9948873266941017, 'epsilon': 2.1023308743480125e-07, 'hidden_layer_sizes': 118, 'learning_rate_init': 0.0018781738757161913, 'tol': 1.1889379831773004e-05, 'validation_fraction': 0.6264327093752792}, {'alpha': 0.04705159350400542, 'batch_size': 158, 'beta_1': 0.9870884262730957, 'beta_2': 0.9999596874382349, 'epsilon': 1.1981845126013875e-08, 'hidden_layer_sizes': 116, 'learning_rate_init': 0.00617340520407431, 'tol': 1.741413418158619e-05, 'validation_fraction': 0.6754299026638921}], [-0.21081107239643826, -0.1295489740611692, -0.27999177313201706, -0.9422522260936896, -0.9332050909794812])}
saving results
saving timing
saving suggest log
done
