{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9a5d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 4.21.12\n",
      "Uninstalling protobuf-4.21.12:\n",
      "  Successfully uninstalled protobuf-4.21.12\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting protobuf==3.20.0\n",
      "  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall protobuf -y && pip3 install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25955b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import acos\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler\n",
    "from raise_utils.data import DataLoader\n",
    "from raise_utils.hooks import Hook\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff07f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb54b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dic_wang = {\"ivy1\":     [\"ivy-1.4.csv\", \"ivy-2.0.csv\"],\n",
    "                 \"lucene1\":  [\"lucene-2.0.csv\", \"lucene-2.2.csv\"],\n",
    "                 \"lucene2\": [\"lucene-2.2.csv\", \"lucene-2.4.csv\"],\n",
    "                 \"poi1\":     [\"poi-1.5.csv\", \"poi-2.5.csv\"],\n",
    "                 \"poi2\": [\"poi-2.5.csv\", \"poi-3.0.csv\"],\n",
    "                 \"synapse1\": [\"synapse-1.0.csv\", \"synapse-1.1.csv\"],\n",
    "                 \"synapse2\": [\"synapse-1.1.csv\", \"synapse-1.2.csv\"],\n",
    "                 \"camel1\": [\"camel-1.2.csv\", \"camel-1.4.csv\"],\n",
    "                 \"camel2\": [\"camel-1.4.csv\", \"camel-1.6.csv\"],\n",
    "                 \"xerces1\": [\"xerces-1.2.csv\", \"xerces-1.3.csv\"],\n",
    "                 \"jedit1\": [\"jedit-3.2.csv\", \"jedit-4.0.csv\"],\n",
    "                 \"jedit2\": [\"jedit-4.0.csv\", \"jedit-4.1.csv\"],\n",
    "                 \"log4j1\": [\"log4j-1.0.csv\", \"log4j-1.1.csv\"],\n",
    "                 \"xalan1\": [\"xalan-2.4.csv\", \"xalan-2.5.csv\"]\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1851ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name: str):\n",
    "    # For the Wang et al. experiments\n",
    "    base_path = './DODGE Data/defect/'\n",
    "    \n",
    "    def _binarize(x, y): y[y > 1] = 1\n",
    "    dataset = DataLoader.from_files(\n",
    "        base_path=base_path, files=file_dic_wang[name], hooks=[Hook('binarize', _binarize)])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c743e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastmap(X, d):\n",
    "    \"\"\"\n",
    "    Perform FastMap dimensionality reduction on the dataset X,\n",
    "    projecting it onto d dimensions.\n",
    "    \"\"\"\n",
    "    n, m = X.shape\n",
    "    Y = np.zeros((n, d))\n",
    "    for i in range(d):\n",
    "        # Select two points at random\n",
    "        p, q = np.random.choice(n, 2, replace=False)\n",
    "        # Compute the distance from each point to the line through p and q\n",
    "        dists = np.abs(np.cross(X[p] - X[q], X - X[q])) / np.linalg.norm(X[p] - X[q])\n",
    "        # Normalize the distances\n",
    "        dists /= np.sum(dists)\n",
    "        # Use the distances as weights to compute the projection onto the line\n",
    "        Y[:, i] = dists * (X[p] - X[q]) + X[q]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bcc985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def furthest_point(p, X):\n",
    "    \"\"\"\n",
    "    Find the furthest point from p in X.\n",
    "    \"\"\"\n",
    "    furthest = None\n",
    "    furthest_distance = 0.\n",
    "    \n",
    "    for q in X:\n",
    "        dist = np.linalg.norm(p - q)\n",
    "        if dist > furthest_distance:\n",
    "            furthest_distance = dist\n",
    "            furthest = q\n",
    "    \n",
    "    return furthest, furthest_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "333ffe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_explained_by_first_component(dataset: str):\n",
    "    data = get_data(dataset)\n",
    "    data.x_train = np.array(data.x_train)\n",
    "    \n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(data.x_train)\n",
    "    \n",
    "    # Use FastMap to find two furthest points\n",
    "    point_idx = np.random.randint(0, len(data.x_train[0]))\n",
    "    point = data.x_train[point_idx]\n",
    "        \n",
    "    f1, d1 = furthest_point(point, data.x_train)\n",
    "    f2, d2 = furthest_point(f1, data.x_train)\n",
    "    \n",
    "    # Find points along principal diagonal\n",
    "    p1 = np.array([0] * len(data.x_train[0]))\n",
    "    p2 = np.array([1] * len(data.x_train[0]))\n",
    "    \n",
    "    # Find the direction vectors\n",
    "    dv1 = (f2 - f1) / np.linalg.norm(f2 - f1)\n",
    "    dv2 = (p2 - p1) / np.linalg.norm(p2 - p1)\n",
    "    \n",
    "    angle = acos(np.dot(dv1, dv2))\n",
    "    \n",
    "    return pca.explained_variance_ratio_[0], angle * 180 / 3.14159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6bd9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ivy1: 0.97 | 108.18574983095587\n",
      "lucene1: 0.92 | 105.34288705705161\n",
      "lucene2: 0.91 | 105.33535289483842\n",
      "poi1: 0.85 | 108.33929027774872\n",
      "poi2: 0.76 | 104.31503993575541\n",
      "synapse1: 0.69 | 111.45388281794523\n",
      "synapse2: 0.77 | 112.00828665490202\n",
      "camel1: 0.87 | 106.41303871510105\n",
      "camel2: 0.93 | 105.81946479776958\n",
      "xerces1: 0.96 | 105.83582314130184\n",
      "jedit1: 0.8 | 103.92822101034967\n",
      "jedit2: 0.74 | 103.88201299638142\n",
      "log4j1: 0.92 | 108.08379832522536\n",
      "xalan1: 0.76 | 109.51023532965712\n"
     ]
    }
   ],
   "source": [
    "for dataset in file_dic_wang:\n",
    "    explained_var, angle = get_percent_explained_by_first_component(dataset)\n",
    "    \n",
    "    print(f'{dataset}: {round(explained_var, 2)} | {angle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7034ce0",
   "metadata": {},
   "source": [
    "This means a majority of the variance is explained in one dimension. If this dimension is along the principal diagonal, it would explain why rwfo and wfo perform similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b62b6",
   "metadata": {},
   "source": [
    "What happens if we do the same but normalize first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1269165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_explained_by_first_component_normalized(dataset: str, scaler=Normalizer):\n",
    "    data = get_data(dataset)\n",
    "    data.x_train = np.array(data.x_train)\n",
    "    \n",
    "    norm = scaler()\n",
    "    data.x_train = norm.fit_transform(data.x_train)\n",
    "    \n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(data.x_train)\n",
    "    \n",
    "    # Use FastMap to find two furthest points\n",
    "    point_idx = np.random.randint(0, len(data.x_train[0]))\n",
    "    point = data.x_train[point_idx]\n",
    "        \n",
    "    f1, d1 = furthest_point(point, data.x_train)\n",
    "    f2, d2 = furthest_point(f1, data.x_train)\n",
    "    \n",
    "    # Find points along principal diagonal\n",
    "    p1 = np.array([0] * len(data.x_train[0]))\n",
    "    p2 = np.array([1] * len(data.x_train[0]))\n",
    "    \n",
    "    # Find the direction vectors\n",
    "    dv1 = (f2 - f1) / np.linalg.norm(f2 - f1)\n",
    "    dv2 = (p2 - p1) / np.linalg.norm(p2 - p1)\n",
    "    \n",
    "    angle = acos(np.dot(dv1, dv2))\n",
    "    \n",
    "    return pca.explained_variance_ratio_[0], angle * 180 / 3.14159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a1bbc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ivy1: 0.6 | 93.12801599929175\n",
      "lucene1: 0.6 | 95.2554764535607\n",
      "lucene2: 0.6 | 95.85701023508356\n",
      "poi1: 0.5 | 91.20187021788193\n",
      "poi2: 0.51 | 96.2339134290284\n",
      "synapse1: 0.66 | 93.93385281276142\n",
      "synapse2: 0.64 | 87.4965175172689\n",
      "camel1: 0.48 | 89.95204221923545\n",
      "camel2: 0.49 | 86.46709528203694\n",
      "xerces1: 0.54 | 94.47486960950138\n",
      "jedit1: 0.51 | 93.63378260676038\n",
      "jedit2: 0.5 | 93.37144467909873\n",
      "log4j1: 0.61 | 95.75339758123273\n",
      "xalan1: 0.46 | 97.60283971531152\n"
     ]
    }
   ],
   "source": [
    "for dataset in file_dic_wang:\n",
    "    explained_var, angle = get_percent_explained_by_first_component_normalized(dataset)\n",
    "    \n",
    "    print(f'{dataset}: {round(explained_var, 2)} | {angle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b722c8",
   "metadata": {},
   "source": [
    "The first PC is roughly orthogonal to the direction that wfo adds samples. This could mean that part of the reason wfo improves results is that it adds samples along a direction where there aren't many to begin with, so the learner's uncertainty is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a22bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
