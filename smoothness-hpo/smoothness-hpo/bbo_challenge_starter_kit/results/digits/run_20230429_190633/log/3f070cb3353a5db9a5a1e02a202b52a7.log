running: {'--uuid': '3f070cb3353a5db9a5a1e02a202b52a7', '-db-root': '/home/ryedida/bbo_challenge_starter_kit/output', '--opt-root': '/home/ryedida/bbo_challenge_starter_kit/example_submissions', '--data-root': None, '--db': 'run_20230429_190633', '--opt': 'hyperopt', '--data': 'digits', '--classifier': 'MLP-adam', '--metric': 'acc', '--calls': 15, '--suggestions': 1, '--jobs-file': None, '--verbose': False, 'dry_run': False, 'rev': 'a376313', 'opt_rev': '0.2.7'}
cmd: python hyperopt/optimizer.py -c MLP-adam -d digits -o hyperopt -u 3f070cb3353a5db9a5a1e02a202b52a7 -m acc -n 15 -p 1 -dir /home/ryedida/bbo_challenge_starter_kit/output -b run_20230429_190633
/home/ryedida/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:605: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

/home/ryedida/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  warnings.warn(

computed signature: ([{'alpha': 0.019628224813442792, 'batch_size': 182, 'beta_1': 0.9410202200271762, 'beta_2': 0.9998021557676793, 'epsilon': 1.8662266976518e-08, 'hidden_layer_sizes': 147, 'learning_rate_init': 0.0005627932047415167, 'tol': 0.03690557729213761, 'validation_fraction': 0.8846827852548593}, {'alpha': 0.0019982467392329444, 'batch_size': 200, 'beta_1': 0.919111482530466, 'beta_2': 0.9998488260436156, 'epsilon': 5.981221901152555e-07, 'hidden_layer_sizes': 61, 'learning_rate_init': 2.2310905607443014e-05, 'tol': 1.2046852412030316e-05, 'validation_fraction': 0.8117896445826539}, {'alpha': 0.46659545670218433, 'batch_size': 219, 'beta_1': 0.9889789783750891, 'beta_2': 0.9999896868093284, 'epsilon': 2.4234724484675948e-08, 'hidden_layer_sizes': 167, 'learning_rate_init': 2.972334644335654e-05, 'tol': 0.0036281404040243792, 'validation_fraction': 0.17260651658522078}, {'alpha': 4.656005689076002, 'batch_size': 135, 'beta_1': 0.870503881627747, 'beta_2': 0.9948873266941017, 'epsilon': 2.1023308743480125e-07, 'hidden_layer_sizes': 118, 'learning_rate_init': 0.0018781738757161913, 'tol': 1.1889379831773004e-05, 'validation_fraction': 0.6264327093752792}, {'alpha': 0.04705159350400542, 'batch_size': 158, 'beta_1': 0.9870884262730957, 'beta_2': 0.9999596874382349, 'epsilon': 1.1981845126013875e-08, 'hidden_layer_sizes': 116, 'learning_rate_init': 0.00617340520407431, 'tol': 1.741413418158619e-05, 'validation_fraction': 0.6754299026638921}], [-0.21081107239643826, -0.1295489740611692, -0.27999177313201706, -0.9422522260936896, -0.9332050909794812])
/home/ryedida/.local/lib/python3.9/site-packages/bayesmark/signatures.py:85: RuntimeWarning: Signature diverged on MLP-adam_digits_acc betwen [-0.2181506  -0.12954897 -0.42701514 -0.94997127 -0.93320509] and [-0.21081107 -0.11575494 -0.27999177 -0.94225223 -0.93253861]
  warnings.warn(

Signature errors:
                           0         1         2         3         4       max
MLP-adam_digits_acc  0.00734  0.013794  0.147023  0.007719  0.000666  0.147023
max                  0.00734  0.013794  0.147023  0.007719  0.000666  0.147023
starting sklearn study hyperopt MLP-adam digits acc 15 1
with data root: None
suggestion time taken 0.002403 iter 0 next_points [{'alpha': 0.018051783425521266, 'batch_size': 177, 'beta_1': 0.9104070843861697, 'beta_2': 0.9471555918410173, 'epsilon': 6.350514871964755e-07, 'hidden_layer_sizes': 143, 'learning_rate_init': 0.0001359885955563031, 'tol': 0.03437696946011472, 'validation_fraction': 0.33323970805339553}]
function_evaluation time 0.754395 value -0.598207 suggestion {'alpha': 0.018051783425521266, 'batch_size': 177, 'beta_1': 0.9104070843861697, 'beta_2': 0.9471555918410173, 'epsilon': 6.350514871964755e-07, 'hidden_layer_sizes': 143, 'learning_rate_init': 0.0001359885955563031, 'tol': 0.03437696946011472, 'validation_fraction': 0.33323970805339553}
observation time 0.000077, current best -0.598207 at iter 0
suggestion time taken 0.002405 iter 1 next_points [{'alpha': 0.18221984325309276, 'batch_size': 103, 'beta_1': 0.5137921591792076, 'beta_2': 0.9034869236691688, 'epsilon': 1.2034121769870303e-09, 'hidden_layer_sizes': 69, 'learning_rate_init': 0.018931438184110052, 'tol': 0.003144099179783969, 'validation_fraction': 0.22476569068068974}]
function_evaluation time 0.783709 value -0.957559 suggestion {'alpha': 0.18221984325309276, 'batch_size': 103, 'beta_1': 0.5137921591792076, 'beta_2': 0.9034869236691688, 'epsilon': 1.2034121769870303e-09, 'hidden_layer_sizes': 69, 'learning_rate_init': 0.018931438184110052, 'tol': 0.003144099179783969, 'validation_fraction': 0.22476569068068974}
observation time 0.000071, current best -0.957559 at iter 1
suggestion time taken 0.002157 iter 2 next_points [{'alpha': 0.0004378476782776119, 'batch_size': 192, 'beta_1': 0.5802186021474517, 'beta_2': 0.9876489217776395, 'epsilon': 4.2819083771732616e-08, 'hidden_layer_sizes': 181, 'learning_rate_init': 6.32292899431673e-05, 'tol': 0.0011974550082570169, 'validation_fraction': 0.401731921984419}]
/home/ryedida/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  warnings.warn(

function_evaluation time 5.228204 value -0.943622 suggestion {'alpha': 0.0004378476782776119, 'batch_size': 192, 'beta_1': 0.5802186021474517, 'beta_2': 0.9876489217776395, 'epsilon': 4.2819083771732616e-08, 'hidden_layer_sizes': 181, 'learning_rate_init': 6.32292899431673e-05, 'tol': 0.0011974550082570169, 'validation_fraction': 0.401731921984419}
observation time 0.000069, current best -0.957559 at iter 2
suggestion time taken 0.002267 iter 3 next_points [{'alpha': 3.856042707967969, 'batch_size': 71, 'beta_1': 0.9535788134339087, 'beta_2': 0.9266165637675761, 'epsilon': 5.242933223517354e-09, 'hidden_layer_sizes': 99, 'learning_rate_init': 0.0018866529984443339, 'tol': 5.749497454918697e-05, 'validation_fraction': 0.29335773363169615}]
function_evaluation time 1.330488 value -0.965212 suggestion {'alpha': 3.856042707967969, 'batch_size': 71, 'beta_1': 0.9535788134339087, 'beta_2': 0.9266165637675761, 'epsilon': 5.242933223517354e-09, 'hidden_layer_sizes': 99, 'learning_rate_init': 0.0018866529984443339, 'tol': 5.749497454918697e-05, 'validation_fraction': 0.29335773363169615}
observation time 0.000067, current best -0.965212 at iter 3
suggestion time taken 0.002384 iter 4 next_points [{'alpha': 0.0001045407293803964, 'batch_size': 75, 'beta_1': 0.957102322011396, 'beta_2': 0.9076529684019702, 'epsilon': 4.312447873518837e-09, 'hidden_layer_sizes': 159, 'learning_rate_init': 0.09391252247905368, 'tol': 0.0042017646742408665, 'validation_fraction': 0.6047834393599583}]
function_evaluation time 0.910747 value -0.410063 suggestion {'alpha': 0.0001045407293803964, 'batch_size': 75, 'beta_1': 0.957102322011396, 'beta_2': 0.9076529684019702, 'epsilon': 4.312447873518837e-09, 'hidden_layer_sizes': 159, 'learning_rate_init': 0.09391252247905368, 'tol': 0.0042017646742408665, 'validation_fraction': 0.6047834393599583}
observation time 0.000073, current best -0.965212 at iter 4
suggestion time taken 0.002158 iter 5 next_points [{'alpha': 4.452238222045197e-05, 'batch_size': 53, 'beta_1': 0.8947588856966076, 'beta_2': 0.9610058697272068, 'epsilon': 3.341615917180053e-08, 'hidden_layer_sizes': 187, 'learning_rate_init': 0.0075387114709846905, 'tol': 0.00018448072801392998, 'validation_fraction': 0.21889225310973512}]
function_evaluation time 1.791896 value -0.963117 suggestion {'alpha': 4.452238222045197e-05, 'batch_size': 53, 'beta_1': 0.8947588856966076, 'beta_2': 0.9610058697272068, 'epsilon': 3.341615917180053e-08, 'hidden_layer_sizes': 187, 'learning_rate_init': 0.0075387114709846905, 'tol': 0.00018448072801392998, 'validation_fraction': 0.21889225310973512}
observation time 0.000072, current best -0.965212 at iter 5
suggestion time taken 0.002421 iter 6 next_points [{'alpha': 0.1979833207768715, 'batch_size': 46, 'beta_1': 0.5191830103555878, 'beta_2': 0.9939449256016563, 'epsilon': 4.355954890438943e-08, 'hidden_layer_sizes': 81, 'learning_rate_init': 0.09783708554082082, 'tol': 0.00013526482959714525, 'validation_fraction': 0.47111444794762475}]
function_evaluation time 0.463210 value -0.347837 suggestion {'alpha': 0.1979833207768715, 'batch_size': 46, 'beta_1': 0.5191830103555878, 'beta_2': 0.9939449256016563, 'epsilon': 4.355954890438943e-08, 'hidden_layer_sizes': 81, 'learning_rate_init': 0.09783708554082082, 'tol': 0.00013526482959714525, 'validation_fraction': 0.47111444794762475}
observation time 0.000082, current best -0.965212 at iter 6
suggestion time taken 0.002231 iter 7 next_points [{'alpha': 2.565418950555509e-05, 'batch_size': 190, 'beta_1': 0.594543398569496, 'beta_2': 0.9790055035912455, 'epsilon': 2.7282197460502798e-08, 'hidden_layer_sizes': 121, 'learning_rate_init': 0.004023689218036781, 'tol': 7.424514736011871e-05, 'validation_fraction': 0.8616768682739717}]
function_evaluation time 0.940737 value -0.861539 suggestion {'alpha': 2.565418950555509e-05, 'batch_size': 190, 'beta_1': 0.594543398569496, 'beta_2': 0.9790055035912455, 'epsilon': 2.7282197460502798e-08, 'hidden_layer_sizes': 121, 'learning_rate_init': 0.004023689218036781, 'tol': 7.424514736011871e-05, 'validation_fraction': 0.8616768682739717}
observation time 0.000073, current best -0.965212 at iter 7
suggestion time taken 0.002148 iter 8 next_points [{'alpha': 0.008975959015799815, 'batch_size': 54, 'beta_1': 0.8372090668944251, 'beta_2': 0.9578390140707506, 'epsilon': 4.673256115564387e-09, 'hidden_layer_sizes': 78, 'learning_rate_init': 0.00022263126335309537, 'tol': 4.630177566028405e-05, 'validation_fraction': 0.11392713624104092}]
function_evaluation time 1.806355 value -0.942942 suggestion {'alpha': 0.008975959015799815, 'batch_size': 54, 'beta_1': 0.8372090668944251, 'beta_2': 0.9578390140707506, 'epsilon': 4.673256115564387e-09, 'hidden_layer_sizes': 78, 'learning_rate_init': 0.00022263126335309537, 'tol': 4.630177566028405e-05, 'validation_fraction': 0.11392713624104092}
observation time 0.000073, current best -0.965212 at iter 8
suggestion time taken 0.002134 iter 9 next_points [{'alpha': 1.9362519094705468e-05, 'batch_size': 45, 'beta_1': 0.7090472627557476, 'beta_2': 0.980849945062667, 'epsilon': 2.1625891866078448e-09, 'hidden_layer_sizes': 89, 'learning_rate_init': 0.0685418229512071, 'tol': 4.410166345810069e-05, 'validation_fraction': 0.18680335532444753}]
function_evaluation time 1.146832 value -0.781528 suggestion {'alpha': 1.9362519094705468e-05, 'batch_size': 45, 'beta_1': 0.7090472627557476, 'beta_2': 0.980849945062667, 'epsilon': 2.1625891866078448e-09, 'hidden_layer_sizes': 89, 'learning_rate_init': 0.0685418229512071, 'tol': 4.410166345810069e-05, 'validation_fraction': 0.18680335532444753}
observation time 0.000071, current best -0.965212 at iter 9
suggestion time taken 0.002159 iter 10 next_points [{'alpha': 0.39854164145886517, 'batch_size': 220, 'beta_1': 0.5264077395652405, 'beta_2': 0.9784619191014018, 'epsilon': 2.199034667799308e-09, 'hidden_layer_sizes': 93, 'learning_rate_init': 0.011024170103182995, 'tol': 1.0962656192971924e-05, 'validation_fraction': 0.5005990713144767}]
function_evaluation time 0.587270 value -0.951982 suggestion {'alpha': 0.39854164145886517, 'batch_size': 220, 'beta_1': 0.5264077395652405, 'beta_2': 0.9784619191014018, 'epsilon': 2.199034667799308e-09, 'hidden_layer_sizes': 93, 'learning_rate_init': 0.011024170103182995, 'tol': 1.0962656192971924e-05, 'validation_fraction': 0.5005990713144767}
observation time 0.000072, current best -0.965212 at iter 10
suggestion time taken 0.002176 iter 11 next_points [{'alpha': 0.023830639920116296, 'batch_size': 73, 'beta_1': 0.9087525286256913, 'beta_2': 0.9971763352346574, 'epsilon': 3.843723550355182e-08, 'hidden_layer_sizes': 177, 'learning_rate_init': 0.005871211587342699, 'tol': 0.0006677094027074502, 'validation_fraction': 0.7657241509185234}]
function_evaluation time 0.724902 value -0.929726 suggestion {'alpha': 0.023830639920116296, 'batch_size': 73, 'beta_1': 0.9087525286256913, 'beta_2': 0.9971763352346574, 'epsilon': 3.843723550355182e-08, 'hidden_layer_sizes': 177, 'learning_rate_init': 0.005871211587342699, 'tol': 0.0006677094027074502, 'validation_fraction': 0.7657241509185234}
observation time 0.000074, current best -0.965212 at iter 11
suggestion time taken 0.002148 iter 12 next_points [{'alpha': 0.00015367816760371433, 'batch_size': 149, 'beta_1': 0.8622947413007145, 'beta_2': 0.9374620039800933, 'epsilon': 3.5946317965961834e-08, 'hidden_layer_sizes': 139, 'learning_rate_init': 4.937547046813845e-05, 'tol': 0.0015633265400866114, 'validation_fraction': 0.8741581905748244}]
function_evaluation time 0.426016 value -0.139211 suggestion {'alpha': 0.00015367816760371433, 'batch_size': 149, 'beta_1': 0.8622947413007145, 'beta_2': 0.9374620039800933, 'epsilon': 3.5946317965961834e-08, 'hidden_layer_sizes': 139, 'learning_rate_init': 4.937547046813845e-05, 'tol': 0.0015633265400866114, 'validation_fraction': 0.8741581905748244}
observation time 0.000077, current best -0.965212 at iter 12
suggestion time taken 0.002220 iter 13 next_points [{'alpha': 0.038989088626997064, 'batch_size': 165, 'beta_1': 0.5926620208776125, 'beta_2': 0.9188915090428167, 'epsilon': 2.380830352489198e-08, 'hidden_layer_sizes': 104, 'learning_rate_init': 5.4423243761205066e-05, 'tol': 0.002907724395975029, 'validation_fraction': 0.21363551207157466}]
function_evaluation time 4.055439 value -0.901222 suggestion {'alpha': 0.038989088626997064, 'batch_size': 165, 'beta_1': 0.5926620208776125, 'beta_2': 0.9188915090428167, 'epsilon': 2.380830352489198e-08, 'hidden_layer_sizes': 104, 'learning_rate_init': 5.4423243761205066e-05, 'tol': 0.002907724395975029, 'validation_fraction': 0.21363551207157466}
observation time 0.000077, current best -0.965212 at iter 13
suggestion time taken 0.002204 iter 14 next_points [{'alpha': 3.68517772319301e-05, 'batch_size': 63, 'beta_1': 0.660622910881834, 'beta_2': 0.9552004644869707, 'epsilon': 1.1258887938183241e-09, 'hidden_layer_sizes': 56, 'learning_rate_init': 0.0005346820765336438, 'tol': 0.0016217974277260385, 'validation_fraction': 0.10734706222131415}]
function_evaluation time 1.326919 value -0.952681 suggestion {'alpha': 3.68517772319301e-05, 'batch_size': 63, 'beta_1': 0.660622910881834, 'beta_2': 0.9552004644869707, 'epsilon': 1.1258887938183241e-09, 'hidden_layer_sizes': 56, 'learning_rate_init': 0.0005346820765336438, 'tol': 0.0016217974277260385, 'validation_fraction': 0.10734706222131415}
observation time 0.000077, current best -0.965212 at iter 14
saving meta data: {'args': {'--uuid': '3f070cb3353a5db9a5a1e02a202b52a7', '-db-root': '/home/ryedida/bbo_challenge_starter_kit/output', '--opt-root': '/home/ryedida/bbo_challenge_starter_kit/example_submissions', '--data-root': None, '--db': 'run_20230429_190633', '--opt': 'hyperopt', '--data': 'digits', '--classifier': 'MLP-adam', '--metric': 'acc', '--calls': 15, '--suggestions': 1, '--jobs-file': None, '--verbose': False, 'dry_run': False, 'rev': 'a376313', 'opt_rev': '0.2.7'}, 'signature': ([{'alpha': 0.019628224813442792, 'batch_size': 182, 'beta_1': 0.9410202200271762, 'beta_2': 0.9998021557676793, 'epsilon': 1.8662266976518e-08, 'hidden_layer_sizes': 147, 'learning_rate_init': 0.0005627932047415167, 'tol': 0.03690557729213761, 'validation_fraction': 0.8846827852548593}, {'alpha': 0.0019982467392329444, 'batch_size': 200, 'beta_1': 0.919111482530466, 'beta_2': 0.9998488260436156, 'epsilon': 5.981221901152555e-07, 'hidden_layer_sizes': 61, 'learning_rate_init': 2.2310905607443014e-05, 'tol': 1.2046852412030316e-05, 'validation_fraction': 0.8117896445826539}, {'alpha': 0.46659545670218433, 'batch_size': 219, 'beta_1': 0.9889789783750891, 'beta_2': 0.9999896868093284, 'epsilon': 2.4234724484675948e-08, 'hidden_layer_sizes': 167, 'learning_rate_init': 2.972334644335654e-05, 'tol': 0.0036281404040243792, 'validation_fraction': 0.17260651658522078}, {'alpha': 4.656005689076002, 'batch_size': 135, 'beta_1': 0.870503881627747, 'beta_2': 0.9948873266941017, 'epsilon': 2.1023308743480125e-07, 'hidden_layer_sizes': 118, 'learning_rate_init': 0.0018781738757161913, 'tol': 1.1889379831773004e-05, 'validation_fraction': 0.6264327093752792}, {'alpha': 0.04705159350400542, 'batch_size': 158, 'beta_1': 0.9870884262730957, 'beta_2': 0.9999596874382349, 'epsilon': 1.1981845126013875e-08, 'hidden_layer_sizes': 116, 'learning_rate_init': 0.00617340520407431, 'tol': 1.741413418158619e-05, 'validation_fraction': 0.6754299026638921}], [-0.21081107239643826, -0.1295489740611692, -0.27999177313201706, -0.9422522260936896, -0.9332050909794812])}
saving results
saving timing
saving suggest log
done
