2023-03-26 19:17:25.728111: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[get_model] Running smooth
[get_model] Finished running smooth
[get_model] Running wfo
[get_model] Finished running wfo
[get_model] Running ultrasample:wfo
[get_model] Finished running ultrasample:wfo
2023-03-26 19:17:32.226357: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-03-26 19:17:32.306626: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-03-26 19:17:33.037341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:41:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5
coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2023-03-26 19:17:33.037393: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-03-26 19:17:33.160936: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-03-26 19:17:33.160980: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-03-26 19:17:33.227307: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-03-26 19:17:33.268210: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-03-26 19:17:33.413079: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-03-26 19:17:33.501669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-03-26 19:17:33.522745: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-03-26 19:17:33.531835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2023-03-26 19:17:33.532449: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-26 19:17:33.534656: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-03-26 19:17:33.534887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:41:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5
coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2023-03-26 19:17:33.534908: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-03-26 19:17:33.534922: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-03-26 19:17:33.534934: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-03-26 19:17:33.534944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-03-26 19:17:33.534955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-03-26 19:17:33.534965: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-03-26 19:17:33.534975: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-03-26 19:17:33.534985: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-03-26 19:17:33.535312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2023-03-26 19:17:33.552483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-03-26 19:17:36.676141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-03-26 19:17:36.676494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2023-03-26 19:17:36.676502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2023-03-26 19:17:36.685427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7281 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:41:00.0, compute capability: 7.5)
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 19)]              0         
_________________________________________________________________
layer_0 (Dense)              (None, 10)                200       
_________________________________________________________________
layer_1 (Dense)              (None, 7)                 77        
_________________________________________________________________
encoded (Dense)              (None, 5)                 40        
_________________________________________________________________
layer_2 (Dense)              (None, 7)                 42        
_________________________________________________________________
layer_3 (Dense)              (None, 10)                80        
_________________________________________________________________
decoded (Dense)              (None, 19)                209       
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
_________________________________________________________________
[get_model] Fitting autoencoder
2023-03-26 19:17:37.174960: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-03-26 19:17:37.234417: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2994485000 Hz
Epoch 1/500
2023-03-26 19:17:37.790275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-03-26 19:17:40.450444: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
 1/23 [>.............................] - ETA: 1:14 - loss: 0.052023/23 [==============================] - ETA: 0s - loss: 0.0513  23/23 [==============================] - 3s 2ms/step - loss: 0.0512
Epoch 2/500
 1/23 [>.............................] - ETA: 0s - loss: 0.050723/23 [==============================] - 0s 2ms/step - loss: 0.0500
Epoch 3/500
 1/23 [>.............................] - ETA: 0s - loss: 0.049123/23 [==============================] - 0s 2ms/step - loss: 0.0489
Epoch 4/500
 1/23 [>.............................] - ETA: 0s - loss: 0.048823/23 [==============================] - 0s 2ms/step - loss: 0.0485
Epoch 5/500
 1/23 [>.............................] - ETA: 0s - loss: 0.047123/23 [==============================] - 0s 2ms/step - loss: 0.0479
Epoch 6/500
 1/23 [>.............................] - ETA: 0s - loss: 0.045923/23 [==============================] - 0s 2ms/step - loss: 0.0473
Epoch 7/500
 1/23 [>.............................] - ETA: 0s - loss: 0.046823/23 [==============================] - 0s 2ms/step - loss: 0.0469
Epoch 8/500
 1/23 [>.............................] - ETA: 0s - loss: 0.046623/23 [==============================] - 0s 2ms/step - loss: 0.0465
Epoch 9/500
 1/23 [>.............................] - ETA: 0s - loss: 0.047423/23 [==============================] - 0s 2ms/step - loss: 0.0459
Epoch 10/500
 1/23 [>.............................] - ETA: 0s - loss: 0.046023/23 [==============================] - 0s 2ms/step - loss: 0.0457
Epoch 11/500
 1/23 [>.............................] - ETA: 0s - loss: 0.047923/23 [==============================] - 0s 2ms/step - loss: 0.0458
Epoch 12/500
 1/23 [>.............................] - ETA: 0s - loss: 0.044123/23 [==============================] - 0s 2ms/step - loss: 0.0452
Epoch 13/500
 1/23 [>.............................] - ETA: 0s - loss: 0.044823/23 [==============================] - 0s 2ms/step - loss: 0.0451
Epoch 14/500
 1/23 [>.............................] - ETA: 0s - loss: 0.044523/23 [==============================] - 0s 2ms/step - loss: 0.0450
Epoch 15/500
 1/23 [>.............................] - ETA: 0s - loss: 0.043423/23 [==============================] - 0s 2ms/step - loss: 0.0448
Epoch 16/500
 1/23 [>.............................] - ETA: 0s - loss: 0.044623/23 [==============================] - 0s 2ms/step - loss: 0.0448
Epoch 17/500
 1/23 [>.............................] - ETA: 0s - loss: 0.046623/23 [==============================] - 0s 2ms/step - loss: 0.0450
Epoch 18/500
 1/23 [>.............................] - ETA: 0s - loss: 0.044223/23 [==============================] - 0s 2ms/step - loss: 0.0447
Epoch 19/500
 1/23 [>.............................] - ETA: 0s - loss: 0.045023/23 [==============================] - 0s 2ms/step - loss: 0.0447
Epoch 20/500
 1/23 [>.............................] - ETA: 0s - loss: 0.044123/23 [==============================] - 0s 2ms/step - loss: 0.0442
Epoch 21/500
 1/23 [>.............................] - ETA: 0s - loss: 0.043423/23 [==============================] - 0s 2ms/step - loss: 0.0443
Epoch 22/500
 1/23 [>.............................] - ETA: 0s - loss: 0.044623/23 [==============================] - 0s 2ms/step - loss: 0.0440
Epoch 23/500
 1/23 [>.............................] - ETA: 0s - loss: 0.043223/23 [==============================] - 0s 2ms/step - loss: 0.0439
Epoch 24/500
 1/23 [>.............................] - ETA: 0s - loss: 0.043023/23 [==============================] - 0s 2ms/step - loss: 0.0438
Epoch 25/500
 1/23 [>.............................] - ETA: 0s - loss: 0.045623/23 [==============================] - 0s 2ms/step - loss: 0.0442
Epoch 26/500
 1/23 [>.............................] - ETA: 0s - loss: 0.044723/23 [==============================] - 0s 2ms/step - loss: 0.0439
Epoch 27/500
 1/23 [>.............................] - ETA: 0s - loss: 0.044423/23 [==============================] - 0s 2ms/step - loss: 0.0440
Epoch 28/500
 1/23 [>.............................] - ETA: 0s - loss: 0.043323/23 [==============================] - 0s 2ms/step - loss: 0.0439
Epoch 29/500
 1/23 [>.............................] - ETA: 0s - loss: 0.041723/23 [==============================] - 0s 2ms/step - loss: 0.0437
[get_model] Fit autoencoder
[get_model] Running wfo
[get_model] Finished running wfo
[get_model] Running smooth
[get_model] Finished running smooth
[get_model] Running ultrasample:wfo
[get_model] Finished running ultrasample:wfo
Model: "model_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 19)]              0         
_________________________________________________________________
layer_0 (Dense)              (None, 10)                200       
_________________________________________________________________
layer_1 (Dense)              (None, 7)                 77        
_________________________________________________________________
encoded (Dense)              (None, 5)                 40        
_________________________________________________________________
layer_2 (Dense)              (None, 7)                 42        
_________________________________________________________________
layer_3 (Dense)              (None, 10)                80        
_________________________________________________________________
decoded (Dense)              (None, 19)                209       
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
_________________________________________________________________
[get_model] Fitting autoencoder
Epoch 1/500
 1/23 [>.............................] - ETA: 6s - loss: 0.054023/23 [==============================] - 0s 2ms/step - loss: 0.0528
Epoch 2/500
 1/23 [>.............................] - ETA: 0s - loss: 0.052123/23 [==============================] - 0s 2ms/step - loss: 0.0518
Epoch 3/500
 1/23 [>.............................] - ETA: 0s - loss: 0.051523/23 [==============================] - 0s 2ms/step - loss: 0.0515
Epoch 4/500
 1/23 [>.............................] - ETA: 0s - loss: 0.051323/23 [==============================] - 0s 2ms/step - loss: 0.0511
Epoch 5/500
 1/23 [>.............................] - ETA: 0s - loss: 0.050623/23 [==============================] - 0s 2ms/step - loss: 0.0510
Epoch 6/500
 1/23 [>.............................] - ETA: 0s - loss: 0.050923/23 [==============================] - 0s 2ms/step - loss: 0.0510
Epoch 7/500
 1/23 [>.............................] - ETA: 0s - loss: 0.050523/23 [==============================] - 0s 2ms/step - loss: 0.0507
Epoch 8/500
 1/23 [>.............................] - ETA: 0s - loss: 0.051623/23 [==============================] - 0s 2ms/step - loss: 0.0508
Epoch 9/500
 1/23 [>.............................] - ETA: 0s - loss: 0.051123/23 [==============================] - 0s 2ms/step - loss: 0.0509
Epoch 10/500
 1/23 [>.............................] - ETA: 0s - loss: 0.051523/23 [==============================] - 0s 2ms/step - loss: 0.0507
Epoch 11/500
 1/23 [>.............................] - ETA: 0s - loss: 0.050023/23 [==============================] - 0s 2ms/step - loss: 0.0507
Epoch 12/500
 1/23 [>.............................] - ETA: 0s - loss: 0.050523/23 [==============================] - 0s 2ms/step - loss: 0.0509
Epoch 13/500
 1/23 [>.............................] - ETA: 0s - loss: 0.051323/23 [==============================] - 0s 2ms/step - loss: 0.0506
Epoch 14/500
 1/23 [>.............................] - ETA: 0s - loss: 0.050823/23 [==============================] - 0s 2ms/step - loss: 0.0501
Epoch 15/500
 1/23 [>.............................] - ETA: 0s - loss: 0.038523/23 [==============================] - 0s 2ms/step - loss: 0.0309
Epoch 16/500
 1/23 [>.............................] - ETA: 0s - loss: 0.013223/23 [==============================] - 0s 2ms/step - loss: 0.0105
Epoch 17/500
 1/23 [>.............................] - ETA: 0s - loss: 0.010923/23 [==============================] - 0s 2ms/step - loss: 0.0094
Epoch 18/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008723/23 [==============================] - 0s 2ms/step - loss: 0.0083
Epoch 19/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007323/23 [==============================] - 0s 2ms/step - loss: 0.0078
Epoch 20/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007923/23 [==============================] - 0s 2ms/step - loss: 0.0079
Epoch 21/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007823/23 [==============================] - 0s 2ms/step - loss: 0.0076
Epoch 22/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007523/23 [==============================] - 0s 2ms/step - loss: 0.0075
Epoch 23/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007823/23 [==============================] - 0s 2ms/step - loss: 0.0077
Epoch 24/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008423/23 [==============================] - 0s 2ms/step - loss: 0.0075
Epoch 25/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008023/23 [==============================] - 0s 2ms/step - loss: 0.0077
Epoch 26/500
 1/23 [>.............................] - ETA: 0s - loss: 0.006923/23 [==============================] - 0s 2ms/step - loss: 0.0075
Epoch 27/500
 1/23 [>.............................] - ETA: 0s - loss: 0.006823/23 [==============================] - 0s 2ms/step - loss: 0.0076
Epoch 28/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007123/23 [==============================] - 0s 2ms/step - loss: 0.0074
Epoch 29/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008623/23 [==============================] - 0s 2ms/step - loss: 0.0073
Epoch 30/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007523/23 [==============================] - 0s 2ms/step - loss: 0.0075
Epoch 31/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007223/23 [==============================] - 0s 2ms/step - loss: 0.0076
Epoch 32/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007123/23 [==============================] - 0s 2ms/step - loss: 0.0076
[get_model] Fit autoencoder
[get_model] Running wfo
[get_model] Finished running wfo
[get_model] Running smooth
[get_model] Finished running smooth
[get_model] Running ultrasample:wfo
[get_model] Finished running ultrasample:wfo
Model: "model_6"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_5 (InputLayer)         [(None, 19)]              0         
_________________________________________________________________
layer_0 (Dense)              (None, 10)                200       
_________________________________________________________________
layer_1 (Dense)              (None, 7)                 77        
_________________________________________________________________
encoded (Dense)              (None, 5)                 40        
_________________________________________________________________
layer_2 (Dense)              (None, 7)                 42        
_________________________________________________________________
layer_3 (Dense)              (None, 10)                80        
_________________________________________________________________
decoded (Dense)              (None, 19)                209       
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
_________________________________________________________________
[get_model] Fitting autoencoder
Epoch 1/500
 1/23 [>.............................] - ETA: 6s - loss: 0.049923/23 [==============================] - 0s 2ms/step - loss: 0.0456
Epoch 2/500
 1/23 [>.............................] - ETA: 0s - loss: 0.029723/23 [==============================] - 0s 2ms/step - loss: 0.0255
Epoch 3/500
 1/23 [>.............................] - ETA: 0s - loss: 0.014723/23 [==============================] - 0s 2ms/step - loss: 0.0122
Epoch 4/500
 1/23 [>.............................] - ETA: 0s - loss: 0.010423/23 [==============================] - 0s 2ms/step - loss: 0.0089
Epoch 5/500
 1/23 [>.............................] - ETA: 0s - loss: 0.010423/23 [==============================] - 0s 2ms/step - loss: 0.0084
Epoch 6/500
 1/23 [>.............................] - ETA: 0s - loss: 0.009723/23 [==============================] - 0s 2ms/step - loss: 0.0080
Epoch 7/500
 1/23 [>.............................] - ETA: 0s - loss: 0.006523/23 [==============================] - 0s 2ms/step - loss: 0.0075
Epoch 8/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007523/23 [==============================] - 0s 2ms/step - loss: 0.0074
Epoch 9/500
 1/23 [>.............................] - ETA: 0s - loss: 0.005123/23 [==============================] - 0s 2ms/step - loss: 0.0068
Epoch 10/500
 1/23 [>.............................] - ETA: 0s - loss: 0.006523/23 [==============================] - 0s 2ms/step - loss: 0.0065
Epoch 11/500
 1/23 [>.............................] - ETA: 0s - loss: 0.005623/23 [==============================] - 0s 2ms/step - loss: 0.0061
Epoch 12/500
 1/23 [>.............................] - ETA: 0s - loss: 0.005523/23 [==============================] - 0s 2ms/step - loss: 0.0055
Epoch 13/500
 1/23 [>.............................] - ETA: 0s - loss: 0.005623/23 [==============================] - 0s 2ms/step - loss: 0.0048
Epoch 14/500
 1/23 [>.............................] - ETA: 0s - loss: 0.004523/23 [==============================] - 0s 2ms/step - loss: 0.0042
Epoch 15/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003423/23 [==============================] - 0s 2ms/step - loss: 0.0039
Epoch 16/500
 1/23 [>.............................] - ETA: 0s - loss: 0.004723/23 [==============================] - 0s 2ms/step - loss: 0.0038
Epoch 17/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003923/23 [==============================] - 0s 2ms/step - loss: 0.0036
Epoch 18/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002923/23 [==============================] - 0s 2ms/step - loss: 0.0034
Epoch 19/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003423/23 [==============================] - 0s 2ms/step - loss: 0.0034
Epoch 20/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003023/23 [==============================] - 0s 2ms/step - loss: 0.0033
Epoch 21/500
 1/23 [>.............................] - ETA: 0s - loss: 0.004123/23 [==============================] - 0s 2ms/step - loss: 0.0033
Epoch 22/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003323/23 [==============================] - 0s 2ms/step - loss: 0.0033
Epoch 23/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002523/23 [==============================] - 0s 2ms/step - loss: 0.0031
Epoch 24/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003723/23 [==============================] - 0s 2ms/step - loss: 0.0031
Epoch 25/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002823/23 [==============================] - 0s 2ms/step - loss: 0.0031
Epoch 26/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003023/23 [==============================] - 0s 2ms/step - loss: 0.0030
Epoch 27/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003223/23 [==============================] - 0s 2ms/step - loss: 0.0030
Epoch 28/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002423/23 [==============================] - 0s 2ms/step - loss: 0.0031
Epoch 29/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002523/23 [==============================] - 0s 2ms/step - loss: 0.0029
Epoch 30/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003623/23 [==============================] - 0s 2ms/step - loss: 0.0029
Epoch 31/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003023/23 [==============================] - 0s 2ms/step - loss: 0.0030
Epoch 32/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002823/23 [==============================] - 0s 2ms/step - loss: 0.0029
[get_model] Fit autoencoder
[get_model] Running wfo
[get_model] Finished running wfo
Beta: 0  | Score: 0.682526661197703
Beta: -18664604.349369075  | Score: 0.0
Beta: -489268981990.3221  | Score: 0.0
[get_model] Running ultrasample:wfo
[get_model] Finished running ultrasample:wfo
Model: "model_9"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         [(None, 19)]              0         
_________________________________________________________________
layer_0 (Dense)              (None, 10)                200       
_________________________________________________________________
layer_1 (Dense)              (None, 7)                 77        
_________________________________________________________________
encoded (Dense)              (None, 5)                 40        
_________________________________________________________________
layer_2 (Dense)              (None, 7)                 42        
_________________________________________________________________
layer_3 (Dense)              (None, 10)                80        
_________________________________________________________________
decoded (Dense)              (None, 19)                209       
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
_________________________________________________________________
[get_model] Fitting autoencoder
Epoch 1/500
 1/23 [>.............................] - ETA: 6s - loss: 0.050423/23 [==============================] - 0s 2ms/step - loss: 0.0481
Epoch 2/500
 1/23 [>.............................] - ETA: 0s - loss: 0.040223/23 [==============================] - 0s 2ms/step - loss: 0.0382
Epoch 3/500
 1/23 [>.............................] - ETA: 0s - loss: 0.030223/23 [==============================] - 0s 2ms/step - loss: 0.0279
Epoch 4/500
 1/23 [>.............................] - ETA: 0s - loss: 0.021323/23 [==============================] - 0s 2ms/step - loss: 0.0187
Epoch 5/500
 1/23 [>.............................] - ETA: 0s - loss: 0.011923/23 [==============================] - 0s 2ms/step - loss: 0.0114
Epoch 6/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008023/23 [==============================] - 0s 2ms/step - loss: 0.0083
Epoch 7/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007123/23 [==============================] - 0s 2ms/step - loss: 0.0073
Epoch 8/500
 1/23 [>.............................] - ETA: 0s - loss: 0.006223/23 [==============================] - 0s 2ms/step - loss: 0.0071
Epoch 9/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007223/23 [==============================] - 0s 2ms/step - loss: 0.0071
Epoch 10/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007923/23 [==============================] - 0s 2ms/step - loss: 0.0068
Epoch 11/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007723/23 [==============================] - 0s 2ms/step - loss: 0.0063
Epoch 12/500
 1/23 [>.............................] - ETA: 0s - loss: 0.005723/23 [==============================] - 0s 2ms/step - loss: 0.0055
Epoch 13/500
 1/23 [>.............................] - ETA: 0s - loss: 0.005023/23 [==============================] - 0s 2ms/step - loss: 0.0045
Epoch 14/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003323/23 [==============================] - 0s 2ms/step - loss: 0.0033
Epoch 15/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002923/23 [==============================] - 0s 2ms/step - loss: 0.0030
Epoch 16/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003023/23 [==============================] - 0s 2ms/step - loss: 0.0029
Epoch 17/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003723/23 [==============================] - 0s 2ms/step - loss: 0.0028
Epoch 18/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003123/23 [==============================] - 0s 2ms/step - loss: 0.0027
Epoch 19/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003123/23 [==============================] - 0s 2ms/step - loss: 0.0027
Epoch 20/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002223/23 [==============================] - 0s 2ms/step - loss: 0.0025
Epoch 21/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002623/23 [==============================] - 0s 2ms/step - loss: 0.0025
Epoch 22/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003023/23 [==============================] - 0s 2ms/step - loss: 0.0026
Epoch 23/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003423/23 [==============================] - 0s 2ms/step - loss: 0.0025
Epoch 24/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003423/23 [==============================] - 0s 2ms/step - loss: 0.0023
Epoch 25/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001923/23 [==============================] - 0s 2ms/step - loss: 0.0022
[get_model] Fit autoencoder
[get_model] Running wfo
[get_model] Finished running wfo
Beta: -8510785854907.794  | Score: 0.682526661197703
[get_model] Running smooth
[get_model] Finished running smooth
[get_model] Running wfo
[get_model] Finished running wfo
Beta: -99535870708724.22  | Score: 0.5362853628536286
Email sent! Message ID:
010001872036b63f-4bc80021-50eb-4009-bd58-8d7b4e20156f-000000
Accuracy: 0.682526661197703
Time: 17.420671224594116
