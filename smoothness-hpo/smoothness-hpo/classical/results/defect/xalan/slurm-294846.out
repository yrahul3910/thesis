2023-03-25 22:49:24.779823: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[get_model] Running smooth
[get_model] Finished running smooth
[get_model] Running ultrasample:wfo
[get_model] Finished running ultrasample:wfo
2023-03-25 22:49:31.300597: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-03-25 22:49:31.357967: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-03-25 22:49:32.111906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:41:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5
coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2023-03-25 22:49:32.111982: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-03-25 22:49:32.241426: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-03-25 22:49:32.241505: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-03-25 22:49:32.321214: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-03-25 22:49:32.358182: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-03-25 22:49:32.503115: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-03-25 22:49:32.536250: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-03-25 22:49:32.556132: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-03-25 22:49:32.570995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2023-03-25 22:49:32.571596: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-25 22:49:32.573814: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-03-25 22:49:32.574054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:41:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5
coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2023-03-25 22:49:32.574075: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-03-25 22:49:32.574088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-03-25 22:49:32.574099: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-03-25 22:49:32.574110: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-03-25 22:49:32.574120: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-03-25 22:49:32.574130: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-03-25 22:49:32.574140: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-03-25 22:49:32.574151: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-03-25 22:49:32.574463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2023-03-25 22:49:32.584343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-03-25 22:49:35.616457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-03-25 22:49:35.616832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2023-03-25 22:49:35.616842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2023-03-25 22:49:35.624830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7281 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:41:00.0, compute capability: 7.5)
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 19)]              0         
_________________________________________________________________
layer_0 (Dense)              (None, 10)                200       
_________________________________________________________________
layer_1 (Dense)              (None, 7)                 77        
_________________________________________________________________
encoded (Dense)              (None, 5)                 40        
_________________________________________________________________
layer_2 (Dense)              (None, 7)                 42        
_________________________________________________________________
layer_3 (Dense)              (None, 10)                80        
_________________________________________________________________
decoded (Dense)              (None, 19)                209       
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
_________________________________________________________________
[get_model] Fitting autoencoder
2023-03-25 22:49:36.116322: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-03-25 22:49:36.158946: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2994460000 Hz
Epoch 1/500
2023-03-25 22:49:36.706181: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-03-25 22:49:39.329173: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
 1/23 [>.............................] - ETA: 1:13 - loss: 0.052222/23 [===========================>..] - ETA: 0s - loss: 0.0497  23/23 [==============================] - 3s 2ms/step - loss: 0.0495
Epoch 2/500
 1/23 [>.............................] - ETA: 0s - loss: 0.043223/23 [==============================] - 0s 2ms/step - loss: 0.0410
Epoch 3/500
 1/23 [>.............................] - ETA: 0s - loss: 0.033323/23 [==============================] - 0s 2ms/step - loss: 0.0309
Epoch 4/500
 1/23 [>.............................] - ETA: 0s - loss: 0.023123/23 [==============================] - 0s 2ms/step - loss: 0.0207
Epoch 5/500
 1/23 [>.............................] - ETA: 0s - loss: 0.014623/23 [==============================] - 0s 2ms/step - loss: 0.0132
Epoch 6/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008123/23 [==============================] - 0s 2ms/step - loss: 0.0102
Epoch 7/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008023/23 [==============================] - 0s 2ms/step - loss: 0.0095
Epoch 8/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008223/23 [==============================] - 0s 2ms/step - loss: 0.0097
Epoch 9/500
 1/23 [>.............................] - ETA: 0s - loss: 0.009923/23 [==============================] - 0s 2ms/step - loss: 0.0099
Epoch 10/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008023/23 [==============================] - 0s 2ms/step - loss: 0.0095
Epoch 11/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008323/23 [==============================] - 0s 2ms/step - loss: 0.0092
Epoch 12/500
 1/23 [>.............................] - ETA: 0s - loss: 0.010523/23 [==============================] - 0s 2ms/step - loss: 0.0094
Epoch 13/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008523/23 [==============================] - 0s 2ms/step - loss: 0.0092
Epoch 14/500
 1/23 [>.............................] - ETA: 0s - loss: 0.009323/23 [==============================] - 0s 2ms/step - loss: 0.0092
Epoch 15/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008823/23 [==============================] - 0s 2ms/step - loss: 0.0089
Epoch 16/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008423/23 [==============================] - 0s 2ms/step - loss: 0.0088
Epoch 17/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008223/23 [==============================] - 0s 2ms/step - loss: 0.0086
Epoch 18/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008123/23 [==============================] - 0s 2ms/step - loss: 0.0090
Epoch 19/500
 1/23 [>.............................] - ETA: 0s - loss: 0.010023/23 [==============================] - 0s 2ms/step - loss: 0.0086
Epoch 20/500
 1/23 [>.............................] - ETA: 0s - loss: 0.010023/23 [==============================] - 0s 2ms/step - loss: 0.0089
Epoch 21/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007823/23 [==============================] - 0s 2ms/step - loss: 0.0085
[get_model] Fit autoencoder
[get_model] Running smooth
[get_model] Finished running smooth
[get_model] Running wfo
[get_model] Finished running wfo
[get_model] Running smooth
[get_model] Finished running smooth
[get_model] Running ultrasample:wfo
[get_model] Finished running ultrasample:wfo
Model: "model_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 19)]              0         
_________________________________________________________________
layer_0 (Dense)              (None, 10)                200       
_________________________________________________________________
layer_1 (Dense)              (None, 7)                 77        
_________________________________________________________________
encoded (Dense)              (None, 5)                 40        
_________________________________________________________________
layer_2 (Dense)              (None, 7)                 42        
_________________________________________________________________
layer_3 (Dense)              (None, 10)                80        
_________________________________________________________________
decoded (Dense)              (None, 19)                209       
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
_________________________________________________________________
[get_model] Fitting autoencoder
Epoch 1/500
 1/23 [>.............................] - ETA: 6s - loss: 0.053323/23 [==============================] - 0s 2ms/step - loss: 0.0523
Epoch 2/500
 1/23 [>.............................] - ETA: 0s - loss: 0.051323/23 [==============================] - 0s 2ms/step - loss: 0.0511
Epoch 3/500
 1/23 [>.............................] - ETA: 0s - loss: 0.051123/23 [==============================] - 0s 2ms/step - loss: 0.0508
Epoch 4/500
 1/23 [>.............................] - ETA: 0s - loss: 0.050823/23 [==============================] - 0s 2ms/step - loss: 0.0505
Epoch 5/500
 1/23 [>.............................] - ETA: 0s - loss: 0.049823/23 [==============================] - 0s 2ms/step - loss: 0.0502
Epoch 6/500
 1/23 [>.............................] - ETA: 0s - loss: 0.050623/23 [==============================] - 0s 2ms/step - loss: 0.0499
Epoch 7/500
 1/23 [>.............................] - ETA: 0s - loss: 0.046923/23 [==============================] - 0s 2ms/step - loss: 0.0418
Epoch 8/500
 1/23 [>.............................] - ETA: 0s - loss: 0.029523/23 [==============================] - 0s 2ms/step - loss: 0.0262
Epoch 9/500
 1/23 [>.............................] - ETA: 0s - loss: 0.016323/23 [==============================] - 0s 2ms/step - loss: 0.0136
Epoch 10/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007823/23 [==============================] - 0s 2ms/step - loss: 0.0073
Epoch 11/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007023/23 [==============================] - 0s 2ms/step - loss: 0.0063
Epoch 12/500
 1/23 [>.............................] - ETA: 0s - loss: 0.006823/23 [==============================] - 0s 2ms/step - loss: 0.0059
Epoch 13/500
 1/23 [>.............................] - ETA: 0s - loss: 0.006623/23 [==============================] - 0s 2ms/step - loss: 0.0053
Epoch 14/500
 1/23 [>.............................] - ETA: 0s - loss: 0.004623/23 [==============================] - 0s 2ms/step - loss: 0.0045
Epoch 15/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003323/23 [==============================] - 0s 2ms/step - loss: 0.0039
Epoch 16/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003923/23 [==============================] - 0s 2ms/step - loss: 0.0035
Epoch 17/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003523/23 [==============================] - 0s 2ms/step - loss: 0.0032
Epoch 18/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002623/23 [==============================] - 0s 2ms/step - loss: 0.0029
Epoch 19/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002523/23 [==============================] - 0s 2ms/step - loss: 0.0027
Epoch 20/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002223/23 [==============================] - 0s 2ms/step - loss: 0.0024
Epoch 21/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002523/23 [==============================] - 0s 2ms/step - loss: 0.0024
Epoch 22/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001823/23 [==============================] - 0s 2ms/step - loss: 0.0022
Epoch 23/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002523/23 [==============================] - 0s 2ms/step - loss: 0.0022
Epoch 24/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001923/23 [==============================] - 0s 2ms/step - loss: 0.0021
Epoch 25/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001823/23 [==============================] - 0s 2ms/step - loss: 0.0019
Epoch 26/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001823/23 [==============================] - 0s 2ms/step - loss: 0.0018
Epoch 27/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001923/23 [==============================] - 0s 2ms/step - loss: 0.0016
Epoch 28/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001723/23 [==============================] - 0s 2ms/step - loss: 0.0016
Epoch 29/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001823/23 [==============================] - 0s 2ms/step - loss: 0.0015
Epoch 30/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001423/23 [==============================] - 0s 2ms/step - loss: 0.0014
Epoch 31/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001123/23 [==============================] - 0s 2ms/step - loss: 0.0013
Epoch 32/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001123/23 [==============================] - 0s 2ms/step - loss: 0.0013
Epoch 33/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001323/23 [==============================] - 0s 2ms/step - loss: 0.0013
Epoch 34/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001323/23 [==============================] - 0s 2ms/step - loss: 0.0013
[get_model] Fit autoencoder
[get_model] Running smooth
[get_model] Finished running smooth
[get_model] Running ultrasample:wfo
[get_model] Finished running ultrasample:wfo
Model: "model_6"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_5 (InputLayer)         [(None, 19)]              0         
_________________________________________________________________
layer_0 (Dense)              (None, 10)                200       
_________________________________________________________________
layer_1 (Dense)              (None, 7)                 77        
_________________________________________________________________
encoded (Dense)              (None, 5)                 40        
_________________________________________________________________
layer_2 (Dense)              (None, 7)                 42        
_________________________________________________________________
layer_3 (Dense)              (None, 10)                80        
_________________________________________________________________
decoded (Dense)              (None, 19)                209       
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
_________________________________________________________________
[get_model] Fitting autoencoder
Epoch 1/500
 1/23 [>.............................] - ETA: 6s - loss: 0.053523/23 [==============================] - 0s 2ms/step - loss: 0.0521
Epoch 2/500
 1/23 [>.............................] - ETA: 0s - loss: 0.047723/23 [==============================] - 0s 2ms/step - loss: 0.0462
Epoch 3/500
 1/23 [>.............................] - ETA: 0s - loss: 0.041623/23 [==============================] - 0s 2ms/step - loss: 0.0395
Epoch 4/500
 1/23 [>.............................] - ETA: 0s - loss: 0.033323/23 [==============================] - 0s 2ms/step - loss: 0.0311
Epoch 5/500
 1/23 [>.............................] - ETA: 0s - loss: 0.024123/23 [==============================] - 0s 2ms/step - loss: 0.0219
Epoch 6/500
 1/23 [>.............................] - ETA: 0s - loss: 0.015223/23 [==============================] - 0s 2ms/step - loss: 0.0152
Epoch 7/500
 1/23 [>.............................] - ETA: 0s - loss: 0.012723/23 [==============================] - 0s 2ms/step - loss: 0.0113
Epoch 8/500
 1/23 [>.............................] - ETA: 0s - loss: 0.010823/23 [==============================] - 0s 2ms/step - loss: 0.0100
Epoch 9/500
 1/23 [>.............................] - ETA: 0s - loss: 0.009323/23 [==============================] - 0s 2ms/step - loss: 0.0091
Epoch 10/500
 1/23 [>.............................] - ETA: 0s - loss: 0.008223/23 [==============================] - 0s 2ms/step - loss: 0.0089
Epoch 11/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007423/23 [==============================] - 0s 2ms/step - loss: 0.0084
Epoch 12/500
 1/23 [>.............................] - ETA: 0s - loss: 0.009123/23 [==============================] - 0s 2ms/step - loss: 0.0088
Epoch 13/500
 1/23 [>.............................] - ETA: 0s - loss: 0.009323/23 [==============================] - 0s 2ms/step - loss: 0.0086
Epoch 14/500
 1/23 [>.............................] - ETA: 0s - loss: 0.006923/23 [==============================] - 0s 2ms/step - loss: 0.0081
Epoch 15/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007723/23 [==============================] - 0s 2ms/step - loss: 0.0084
Epoch 16/500
 1/23 [>.............................] - ETA: 0s - loss: 0.006823/23 [==============================] - 0s 2ms/step - loss: 0.0082
Epoch 17/500
 1/23 [>.............................] - ETA: 0s - loss: 0.006123/23 [==============================] - 0s 2ms/step - loss: 0.0060
Epoch 18/500
 1/23 [>.............................] - ETA: 0s - loss: 0.006223/23 [==============================] - 0s 2ms/step - loss: 0.0054
Epoch 19/500
 1/23 [>.............................] - ETA: 0s - loss: 0.004623/23 [==============================] - 0s 2ms/step - loss: 0.0047
Epoch 20/500
 1/23 [>.............................] - ETA: 0s - loss: 0.004023/23 [==============================] - 0s 2ms/step - loss: 0.0040
Epoch 21/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003623/23 [==============================] - 0s 2ms/step - loss: 0.0035
Epoch 22/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002523/23 [==============================] - 0s 2ms/step - loss: 0.0030
Epoch 23/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003023/23 [==============================] - 0s 2ms/step - loss: 0.0029
Epoch 24/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003223/23 [==============================] - 0s 2ms/step - loss: 0.0028
Epoch 25/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002923/23 [==============================] - 0s 2ms/step - loss: 0.0028
Epoch 26/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003423/23 [==============================] - 0s 2ms/step - loss: 0.0030
Epoch 27/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003123/23 [==============================] - 0s 2ms/step - loss: 0.0029
Epoch 28/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002923/23 [==============================] - 0s 2ms/step - loss: 0.0028
Epoch 29/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002723/23 [==============================] - 0s 2ms/step - loss: 0.0027
Epoch 30/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003223/23 [==============================] - 0s 2ms/step - loss: 0.0029
Epoch 31/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003523/23 [==============================] - 0s 2ms/step - loss: 0.0030
[get_model] Fit autoencoder
Beta: 0  | Score: 0.682526661197703
Beta: -489286984899.74493  | Score: 0.0
Beta: -715038985525.3579  | Score: 0.0
[get_model] Running smooth
[get_model] Finished running smooth
[get_model] Running wfo
[get_model] Finished running wfo
Beta: -1116458417950.0698  | Score: 0.0
[get_model] Running smooth
[get_model] Finished running smooth
[get_model] Running ultrasample:wfo
[get_model] Finished running ultrasample:wfo
Model: "model_9"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         [(None, 19)]              0         
_________________________________________________________________
layer_0 (Dense)              (None, 10)                200       
_________________________________________________________________
layer_1 (Dense)              (None, 7)                 77        
_________________________________________________________________
encoded (Dense)              (None, 5)                 40        
_________________________________________________________________
layer_2 (Dense)              (None, 7)                 42        
_________________________________________________________________
layer_3 (Dense)              (None, 10)                80        
_________________________________________________________________
decoded (Dense)              (None, 19)                209       
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
_________________________________________________________________
[get_model] Fitting autoencoder
Epoch 1/500
 1/23 [>.............................] - ETA: 6s - loss: 0.052823/23 [==============================] - 0s 2ms/step - loss: 0.0507
Epoch 2/500
 1/23 [>.............................] - ETA: 0s - loss: 0.043823/23 [==============================] - 0s 2ms/step - loss: 0.0408
Epoch 3/500
 1/23 [>.............................] - ETA: 0s - loss: 0.032223/23 [==============================] - 0s 2ms/step - loss: 0.0293
Epoch 4/500
 1/23 [>.............................] - ETA: 0s - loss: 0.020923/23 [==============================] - 0s 2ms/step - loss: 0.0189
Epoch 5/500
 1/23 [>.............................] - ETA: 0s - loss: 0.012923/23 [==============================] - 0s 2ms/step - loss: 0.0117
Epoch 6/500
 1/23 [>.............................] - ETA: 0s - loss: 0.007823/23 [==============================] - 0s 2ms/step - loss: 0.0072
Epoch 7/500
 1/23 [>.............................] - ETA: 0s - loss: 0.005723/23 [==============================] - 0s 2ms/step - loss: 0.0056
Epoch 8/500
 1/23 [>.............................] - ETA: 0s - loss: 0.005023/23 [==============================] - 0s 2ms/step - loss: 0.0044
Epoch 9/500
 1/23 [>.............................] - ETA: 0s - loss: 0.004723/23 [==============================] - 0s 2ms/step - loss: 0.0039
Epoch 10/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003423/23 [==============================] - 0s 2ms/step - loss: 0.0034
Epoch 11/500
 1/23 [>.............................] - ETA: 0s - loss: 0.004423/23 [==============================] - 0s 2ms/step - loss: 0.0034
Epoch 12/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002423/23 [==============================] - 0s 2ms/step - loss: 0.0028
Epoch 13/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002123/23 [==============================] - 0s 2ms/step - loss: 0.0026
Epoch 14/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001923/23 [==============================] - 0s 2ms/step - loss: 0.0025
Epoch 15/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001523/23 [==============================] - 0s 2ms/step - loss: 0.0021
Epoch 16/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001823/23 [==============================] - 0s 2ms/step - loss: 0.0022
Epoch 17/500
 1/23 [>.............................] - ETA: 0s - loss: 0.003123/23 [==============================] - 0s 2ms/step - loss: 0.0024
Epoch 18/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002023/23 [==============================] - 0s 2ms/step - loss: 0.0021
Epoch 19/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002723/23 [==============================] - 0s 2ms/step - loss: 0.0023
Epoch 20/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001723/23 [==============================] - 0s 2ms/step - loss: 0.0021
Epoch 21/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001223/23 [==============================] - 0s 2ms/step - loss: 0.0019
Epoch 22/500
 1/23 [>.............................] - ETA: 0s - loss: 0.002523/23 [==============================] - 0s 2ms/step - loss: 0.0022
Epoch 23/500
 1/23 [>.............................] - ETA: 0s - loss: 0.001723/23 [==============================] - 0s 2ms/step - loss: 0.0020
[get_model] Fit autoencoder
Beta: -4414679580730.687  | Score: 0.682526661197703
Accuracy: 0.682526661197703
Time: 16.286879539489746
